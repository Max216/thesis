\BOOKMARK [1][-]{section.1}{Introduction}{}% 1
\BOOKMARK [1][-]{section.2}{Theoretical Background}{}% 2
\BOOKMARK [2][-]{subsection.2.1}{Natural Language Inference}{section.2}% 3
\BOOKMARK [2][-]{subsection.2.2}{Lexical Semantic Relations}{section.2}% 4
\BOOKMARK [3][-]{subsubsection.2.2.1}{Synonymy and antonomy}{subsection.2.2}% 5
\BOOKMARK [3][-]{subsubsection.2.2.2}{Hypernomy}{subsection.2.2}% 6
\BOOKMARK [3][-]{subsubsection.2.2.3}{Holonomy}{subsection.2.2}% 7
\BOOKMARK [2][-]{subsection.2.3}{Shortcut-Stacked-Encoder and Residual Encoder}{section.2}% 8
\BOOKMARK [3][-]{subsubsection.2.3.1}{Sentence Encoding for Shortcut-Stacked-Encoder}{subsection.2.3}% 9
\BOOKMARK [3][-]{subsubsection.2.3.2}{Classification}{subsection.2.3}% 10
\BOOKMARK [3][-]{subsubsection.2.3.3}{Training}{subsection.2.3}% 11
\BOOKMARK [3][-]{subsubsection.2.3.4}{Residual Encoder and Reimplementation Variants}{subsection.2.3}% 12
\BOOKMARK [1][-]{section.3}{Related Work}{}% 13
\BOOKMARK [2][-]{subsection.3.1}{External Resources}{section.3}% 14
\BOOKMARK [3][-]{subsubsection.3.1.1}{WordNet}{subsection.3.1}% 15
\BOOKMARK [3][-]{subsubsection.3.1.2}{Wikipedia}{subsection.3.1}% 16
\BOOKMARK [3][-]{subsubsection.3.1.3}{Derived from multiple Knowledge Bases}{subsection.3.1}% 17
\BOOKMARK [2][-]{subsection.3.2}{Datasets for NLI}{section.3}% 18
\BOOKMARK [3][-]{subsubsection.3.2.1}{SNLI}{subsection.3.2}% 19
\BOOKMARK [3][-]{subsubsection.3.2.2}{MultiNLI}{subsection.3.2}% 20
\BOOKMARK [3][-]{subsubsection.3.2.3}{SciTail}{subsection.3.2}% 21
\BOOKMARK [2][-]{subsection.3.3}{Neural Models for NLI}{section.3}% 22
\BOOKMARK [3][-]{subsubsection.3.3.1}{Sentence Encoding Models}{subsection.3.3}% 23
\BOOKMARK [3][-]{subsubsection.3.3.2}{Inter-Sentence-Attention-based Models}{subsection.3.3}% 24
\BOOKMARK [2][-]{subsection.3.4}{Integration of external Resources into Neural Networks}{section.3}% 25
\BOOKMARK [3][-]{subsubsection.3.4.1}{Improving word-embeddings}{subsection.3.4}% 26
\BOOKMARK [1][-]{section.4}{Understanding Shortcut-Stacked-Encoder}{}% 27
\BOOKMARK [2][-]{subsection.4.1}{Motivation}{section.4}% 28
\BOOKMARK [2][-]{subsection.4.2}{Insights on the sentence representation}{section.4}% 29
\BOOKMARK [3][-]{subsubsection.4.2.1}{Approach}{subsection.4.2}% 30
\BOOKMARK [3][-]{subsubsection.4.2.2}{Detection of relevant dimensions}{subsection.4.2}% 31
\BOOKMARK [3][-]{subsubsection.4.2.3}{Dimension-wise Analysis}{subsection.4.2}% 32
\BOOKMARK [3][-]{subsubsection.4.2.4}{Conclusion}{subsection.4.2}% 33
\BOOKMARK [2][-]{subsection.4.3}{Insights on the sentence alignment}{section.4}% 34
\BOOKMARK [3][-]{subsubsection.4.3.1}{Approach}{subsection.4.3}% 35
\BOOKMARK [3][-]{subsubsection.4.3.2}{Entailment analysis}{subsection.4.3}% 36
\BOOKMARK [3][-]{subsubsection.4.3.3}{Neutral and contradiction analysis}{subsection.4.3}% 37
\BOOKMARK [3][-]{subsubsection.4.3.4}{Experiments}{subsection.4.3}% 38
\BOOKMARK [3][-]{subsubsection.4.3.5}{Conclusion}{subsection.4.3}% 39
\BOOKMARK [2][-]{subsection.4.4}{Errors of the base model}{section.4}% 40
\BOOKMARK [1][-]{section.5}{Additional SNLI test-set}{}% 41
\BOOKMARK [2][-]{subsection.5.1}{Motivation}{section.5}% 42
\BOOKMARK [2][-]{subsection.5.2}{Dataset}{section.5}% 43
\BOOKMARK [3][-]{subsubsection.5.2.1}{Creation}{subsection.5.2}% 44
\BOOKMARK [3][-]{subsubsection.5.2.2}{Validation}{subsection.5.2}% 45
\BOOKMARK [3][-]{subsubsection.5.2.3}{Final dataset}{subsection.5.2}% 46
\BOOKMARK [2][-]{subsection.5.3}{Other models}{section.5}% 47
\BOOKMARK [3][-]{subsubsection.5.3.1}{ESIM}{subsection.5.3}% 48
\BOOKMARK [3][-]{subsubsection.5.3.2}{Decomposable Attention}{subsection.5.3}% 49
\BOOKMARK [2][-]{subsection.5.4}{Evaluation}{section.5}% 50
\BOOKMARK [2][-]{subsection.5.5}{Analysis}{section.5}% 51
\BOOKMARK [1][-]{section.6}{Approaches to incorporate WordNet information}{}% 52
\BOOKMARK [2][-]{subsection.6.1}{Extraction of WordNet data}{section.6}% 53
\BOOKMARK [2][-]{subsection.6.2}{Integrating information into word-embeddings}{section.6}% 54
\BOOKMARK [3][-]{subsubsection.6.2.1}{Motivation}{subsection.6.2}% 55
\BOOKMARK [3][-]{subsubsection.6.2.2}{Concatenating pre-trained word-embeddings}{subsection.6.2}% 56
\BOOKMARK [3][-]{subsubsection.6.2.3}{Concatenation categorical information}{subsection.6.2}% 57
\BOOKMARK [3][-]{subsubsection.6.2.4}{Analysis}{subsection.6.2}% 58
\BOOKMARK [2][-]{subsection.6.3}{Multitask Learning}{section.6}% 59
\BOOKMARK [3][-]{subsubsection.6.3.1}{Motivation}{subsection.6.3}% 60
\BOOKMARK [3][-]{subsubsection.6.3.2}{Architecture}{subsection.6.3}% 61
\BOOKMARK [3][-]{subsubsection.6.3.3}{Approaches}{subsection.6.3}% 62
\BOOKMARK [3][-]{subsubsection.6.3.4}{Analysis}{subsection.6.3}% 63
\BOOKMARK [3][-]{subsubsection.6.3.5}{Evaluation}{subsection.6.3}% 64
\BOOKMARK [1][-]{section.7}{Conclusion}{}% 65
