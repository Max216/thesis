\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand*\new@tpo@label[2]{}
\bibstyle{acl_natbib}
\AC@reset@newl@bel
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\babel@aux{english}{}
\newacro{biLSTM}[\AC@hyperlink{biLSTM}{biLSTM}]{bidirectional Long-Short-Term Memory Network}
\newacro{BoW}[\AC@hyperlink{BoW}{BoW}]{Bag of Words}
\newacro{ESIM}[\AC@hyperlink{ESIM}{ESIM}]{Enhanced Sequential Inference Model}
\newacro{IE}[\AC@hyperlink{IE}{IE}]{Information Extraction}
\newacro{IR}[\AC@hyperlink{IR}{IR}]{Information Retrieval}
\newacro{KIM}[\AC@hyperlink{KIM}{KIM}]{Knowledge-based Inference Model}
\newacro{LSTM}[\AC@hyperlink{LSTM}{LSTM}]{Long-Short-Term-Memory}
\newacro{MLP}[\AC@hyperlink{MLP}{MLP}]{Multi Layer Perceptron}
\newacro{MultiNLI}[\AC@hyperlink{MultiNLI}{MultiNLI}]{MultiGenre Natural Language Inference Corpus}
\newacro{NLI}[\AC@hyperlink{NLI}{NLI}]{Natural Language Inference}
\newacro{NLP}[\AC@hyperlink{NLP}{NLP}]{Natural Language Processing}
\newacro{NLU}[\AC@hyperlink{NLU}{NLU}]{Natural Language Understanding}
\newacro{POS}[\AC@hyperlink{POS}{POS}]{Part of Speech}
\newacro{OANC}[\AC@hyperlink{OANC}{OANC}]{Open American National Corpus}
\newacro{QA}[\AC@hyperlink{QA}{QA}]{Question Answering}
\newacro{RNN}[\AC@hyperlink{RNN}{RNN}]{Recurrent Neural Network}
\newacro{RTE}[\AC@hyperlink{RTE}{RTE}]{Recognizing Textual Entailment}
\newacro{SICK}[\AC@hyperlink{SICK}{SICK}]{Sentences Involving Compositional Knowledge}
\newacro{SNLI}[\AC@hyperlink{SNLI}{SNLI}]{The Stanford Natural Language Inference Corpus}
\newacro{WSD}[\AC@hyperlink{WSD}{WSD}]{Word Sense Disambiguation}
\newacro{YAGO}[\AC@hyperlink{YAGO}{YAGO}]{Yet Another Great Ontology}
\citation{bengio2013representation}
\citation{mikolov2013distributed}
\citation{pennington2014glove}
\citation{celikyilmaz2010enriching}
\citation{vulic2017morph}
\citation{bowman2015large}
\citation{dagan2006pascal}
\citation{maccartney2007natural}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{5}{section.1}}
\AC@undonewlabel{acro:NLP}
\newlabel{acro:NLP}{{1}{5}{Introduction}{section*.1}{}}
\acronymused{NLP}
\AC@undonewlabel{acro:NLU}
\newlabel{acro:NLU}{{1}{5}{Introduction}{section*.2}{}}
\acronymused{NLU}
\acronymused{NLU}
\acronymused{NLP}
\AC@undonewlabel{acro:NLI}
\newlabel{acro:NLI}{{1}{5}{Introduction}{section*.3}{}}
\acronymused{NLI}
\AC@undonewlabel{acro:RTE}
\newlabel{acro:RTE}{{1}{5}{Introduction}{section*.4}{}}
\acronymused{RTE}
\acronymused{NLU}
\acronymused{NLP}
\AC@undonewlabel{acro:LSTM}
\newlabel{acro:LSTM}{{1}{5}{}{section*.5}{}}
\acronymused{LSTM}
\AC@undonewlabel{acro:RNN}
\newlabel{acro:RNN}{{1}{5}{}{section*.6}{}}
\acronymused{RNN}
\acronymused{NLP}
\acronymused{NLI}
\acronymused{NLI}
\acronymused{NLU}
\acronymused{NLI}
\citation{bowman2015large}
\citation{dagan2009recognizing}
\citation{maccartney2008phrase}
\citation{maccartney2007natural,bos2005recognising}
\citation{dagan2009recognizing}
\citation{williams2017broad,cooper1996using,bos2005recognising,dagan2006pascal}
\citation{murphy2003semantic}
\citation{dagan2009recognizing}
\citation{Jurafsky2008May}
\citation{Jurafsky2008May}
\@writefile{toc}{\contentsline {section}{\numberline {2}Theoretical Background}{6}{section.2}}
\newlabel{sec:basics}{{2}{6}{Theoretical Background}{section.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Natural Language Inference}{6}{subsection.2.1}}
\newlabel{sec:basics_nli}{{2.1}{6}{Natural Language Inference}{subsection.2.1}{}}
\acronymused{NLI}
\acronymused{NLI}
\acronymused{NLI}
\acronymused{NLI}
\acronymused{NLP}
\acronymused{NLU}
\acronymused{NLP}
\AC@undonewlabel{acro:QA}
\newlabel{acro:QA}{{2.1}{6}{}{section*.7}{}}
\acronymused{QA}
\AC@undonewlabel{acro:IE}
\newlabel{acro:IE}{{2.1}{6}{}{section*.8}{}}
\acronymused{IE}
\acronymused{QA}
\acronymused{IE}
\acronymused{NLP}
\acronymused{NLU}
\acronymused{NLU}
\acronymused{NLI}
\acronymused{NLU}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Lexical Semantic Relations}{6}{subsection.2.2}}
\newlabel{sec:word_relations}{{2.2}{6}{Lexical Semantic Relations}{subsection.2.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.1}Synonymy and antonomy}{6}{subsubsection.2.2.1}}
\citation{Jurafsky2008May}
\citation{Jurafsky2008May}
\citation{nie2017shortcut}
\citation{bromley1994signature}
\citation{nie2017shortcut}
\citation{nie2017shortcut}
\citation{hochreiter1997long}
\citation{graves2005framewise}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces A sample ontology of animals to illustrate the lexical relations \textit  {Hypernomy} and \textit  {Holonymy}.}}{7}{figure.1}}
\newlabel{fig:lexical_resources}{{1}{7}{A sample ontology of animals to illustrate the lexical relations \textit {Hypernomy} and \textit {Holonymy}}{figure.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.2}Hypernomy}{7}{subsubsection.2.2.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.3}Holonomy}{7}{subsubsection.2.2.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Shortcut-Stacked-Encoder and Residual Encoder}{7}{subsection.2.3}}
\newlabel{sec:residual_encoder_def}{{2.3}{7}{Shortcut-Stacked-Encoder and Residual Encoder}{subsection.2.3}{}}
\acronymused{NLI}
\acronymused{NLI}
\AC@undonewlabel{acro:MLP}
\newlabel{acro:MLP}{{2.3}{7}{Shortcut-Stacked-Encoder and Residual Encoder}{section*.9}{}}
\acronymused{MLP}
\citation{nie2017shortcut}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.1}Sentence Encoding for Shortcut-Stacked-Encoder}{8}{subsubsection.2.3.1}}
\AC@undonewlabel{acro:biLSTM}
\newlabel{acro:biLSTM}{{2.3.1}{8}{Sentence Encoding for Shortcut-Stacked-Encoder}{section*.10}{}}
\acronymused{biLSTM}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces The architecture of the sentence-encoding component within the Shortcut-Stacked-Encoder, taken from \cite  {nie2017shortcut}.}}{8}{figure.2}}
\newlabel{fig:sentence_emcoder_shortcut}{{2}{8}{The architecture of the sentence-encoding component within the Shortcut-Stacked-Encoder, taken from \cite {nie2017shortcut}}{figure.2}{}}
\acronymused{LSTM}
\acronymused{biLSTM}
\acronymused{biLSTM}
\acronymused{biLSTM}
\acronymused{biLSTM}
\acronymused{biLSTM}
\acronymused{biLSTM}
\newlabel{eq:stacked_encoder_input}{{2}{8}{Sentence Encoding for Shortcut-Stacked-Encoder}{equation.2.2}{}}
\acronymused{biLSTM}
\acronymused{biLSTM}
\citation{mou2015natural}
\citation{nie2017shortcut}
\citation{kingma2014adam}
\citation{pennington2014glove}
\citation{nie2017shortcut}
\citation{gong2017natural}
\citation{gong2017natural}
\citation{nie2017shortcut}
\citation{nie2017shortcut}
\citation{nie2017shortcut}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.2}Classification}{9}{subsubsection.2.3.2}}
\acronymused{MLP}
\acronymused{MLP}
\acronymused{NLI}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.3}Training}{9}{subsubsection.2.3.3}}
\acronymused{MLP}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.4}Residual Encoder and Reimplementation Variants}{9}{subsubsection.2.3.4}}
\acronymused{biLSTM}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Accuracy in percent of different implementations of the model from \cite  {nie2017shortcut}, achieved on the SNLI dataset compared with human performance.}}{9}{table.1}}
\newlabel{table:reimplementation_performance}{{1}{9}{Accuracy in percent of different implementations of the model from \cite {nie2017shortcut}, achieved on the SNLI dataset compared with human performance}{table.1}{}}
\acronymused{biLSTM}
\acronymused{MLP}
\acronymused{NLI}
\acronymused{biLSTM}
\acronymused{MLP}
\citation{bos2005recognising,tatu2005semantic}
\citation{miller1995wordnet}
\citation{Jurafsky2008May}
\citation{Jurafsky2008May}
\citation{mccarthy2004using}
\citation{resnik1995using}
\citation{prakash2007learning}
\citation{gurevych2016linked}
\citation{zesch2008extracting}
\citation{gurevych2016linked}
\@writefile{toc}{\contentsline {section}{\numberline {3}Related Work}{11}{section.3}}
\newlabel{sec:related_work}{{3}{11}{Related Work}{section.3}{}}
\acronymused{NLI}
\acronymused{NLP}
\acronymused{NLI}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}External Resources}{11}{subsection.3.1}}
\newlabel{sec:ext_resources}{{3.1}{11}{External Resources}{subsection.3.1}{}}
\acronymused{NLI}
\acronymused{NLP}
\acronymused{NLI}
\acronymused{NLU}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.1}WordNet}{11}{subsubsection.3.1.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Example of different synsets of the lemma ``table'' (only noun senses) within WordNet, taken from \href  {http://wordnetweb.princeton.edu}{http://wordnetweb.princeton.edu}.}}{11}{figure.3}}
\newlabel{fig:wordnet}{{3}{11}{Example of different synsets of the lemma ``table'' (only noun senses) within WordNet, taken from \href {http://wordnetweb.princeton.edu}{http://wordnetweb.princeton.edu}}{figure.3}{}}
\AC@undonewlabel{acro:POS}
\newlabel{acro:POS}{{3.1.1}{11}{WordNet}{section*.11}{}}
\acronymused{POS}
\AC@undonewlabel{acro:WSD}
\newlabel{acro:WSD}{{3.1.1}{11}{}{section*.12}{}}
\acronymused{WSD}
\citation{suchanek2007yago}
\citation{gurevych2012uby}
\citation{cooper1996using}
\citation{dagan2006pascal}
\citation{marelli2014semeval}
\citation{young2014image}
\citation{bowman2015large}
\citation{bowman2015large}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.2}Wikipedia}{12}{subsubsection.3.1.2}}
\acronymused{NLP}
\acronymused{NLI}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.3}Derived from multiple Knowledge Bases}{12}{subsubsection.3.1.3}}
\AC@undonewlabel{acro:YAGO}
\newlabel{acro:YAGO}{{3.1.3}{12}{Derived from multiple Knowledge Bases}{section*.13}{}}
\acronymused{YAGO}
\acronymused{YAGO}
\acronymused{NLP}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Datasets for NLI}{12}{subsection.3.2}}
\newlabel{sec:basics_datasets}{{3.2}{12}{Datasets for NLI}{subsection.3.2}{}}
\acronymused{NLI}
\AC@undonewlabel{acro:SNLI}
\newlabel{acro:SNLI}{{3.2}{12}{Datasets for NLI}{section*.14}{}}
\acronymused{SNLI}
\acronymused{NLI}
\AC@undonewlabel{acro:SICK}
\newlabel{acro:SICK}{{3.2}{12}{Datasets for NLI}{section*.15}{}}
\acronymused{SICK}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1}SNLI}{12}{subsubsection.3.2.1}}
\acronymused{SNLI}
\acronymused{NLI}
\acronymused{NLI}
\citation{bowman2015large}
\citation{young2014image}
\citation{gururangan2018annotation}
\acronymused{SNLI}
\acronymused{SNLI}
\acronymused{SNLI}
\acronymused{SNLI}
\acronymused{SNLI}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Example sentence pairs, taken from \ac {SNLI}, showing typical sentences within the dataset.}}{13}{table.2}}
\acronymused{SNLI}
\newlabel{table:snli_example}{{2}{13}{Example sentence pairs, taken from \ac {SNLI}, showing typical sentences within the dataset}{table.2}{}}
\acronymused{SNLI}
\citation{chatzikyriakidis2017overview,williams2017broad}
\citation{williams2017broad}
\citation{bowman2015large}
\citation{ide2001american,ide2004american,ide2006integrating}
\citation{nangia2017repeval}
\citation{williams2017broad}
\citation{chen2017recurrent}
\citation{nie2017shortcut}
\citation{nie2017shortcut,balazs2017refining,yang2017character}
\citation{scitail}
\AC@undonewlabel{acro:BoW}
\newlabel{acro:BoW}{{3.2.1}{14}{}{section*.16}{}}
\acronymused{BoW}
\acronymused{SNLI}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.2}MultiNLI}{14}{subsubsection.3.2.2}}
\acronymused{SNLI}
\acronymused{SNLI}
\AC@undonewlabel{acro:MultiNLI}
\newlabel{acro:MultiNLI}{{3.2.2}{14}{MultiNLI}{section*.17}{}}
\acronymused{MultiNLI}
\acronymused{MultiNLI}
\AC@undonewlabel{acro:OANC}
\newlabel{acro:OANC}{{3.2.2}{14}{}{section*.18}{}}
\acronymused{OANC}
\acronymused{MultiNLI}
\acronymused{NLI}
\acronymused{NLU}
\acronymused{SNLI}
\acronymused{NLU}
\acronymused{MultiNLI}
\acronymused{SNLI}
\acronymused{SNLI}
\acronymused{MultiNLI}
\acronymused{SNLI}
\acronymused{MultiNLI}
\acronymused{OANC}
\citation{scitail}
\citation{welbl2017crowdsourcing}
\citation{scitail}
\citation{clark2016combining}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Example sentence pairs from \ac {MultiNLI}, taken from RepEval 2017 Shared Task, showing samples of different genres.}}{15}{table.3}}
\acronymused{MultiNLI}
\newlabel{table:multinli_example}{{3}{15}{Example sentence pairs from \ac {MultiNLI}, taken from RepEval 2017 Shared Task, showing samples of different genres}{table.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.3}SciTail}{15}{subsubsection.3.2.3}}
\acronymused{NLI}
\acronymused{QA}
\acronymused{SNLI}
\acronymused{MultiNLI}
\acronymused{NLI}
\acronymused{QA}
\AC@undonewlabel{acro:IR}
\newlabel{acro:IR}{{2}{15}{}{section*.19}{}}
\acronymused{IR}
\acronymused{SNLI}
\acronymused{MultiNLI}
\acronymused{QA}
\acronymused{NLI}
\acronymused{QA}
\acronymused{NLU}
\citation{bromley1994signature}
\citation{bowman2016fast}
\citation{bowman2016fast}
\citation{munkhdalai2017neural}
\citation{chen2017recurrent}
\citation{nie2017shortcut}
\citation{shen2018reinforced,im2017distance}
\citation{shen2018reinforced}
\citation{shen2018reinforced}
\citation{shen2018reinforced}
\citation{im2017distance}
\citation{rocktaschel2015reasoning}
\citation{cheng2016long}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Example sentence pairs from SciTail Task, different premises retrieved for two hypothesis.}}{16}{table.4}}
\newlabel{table:scitail_example}{{4}{16}{Example sentence pairs from SciTail Task, different premises retrieved for two hypothesis}{table.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Neural Models for NLI}{16}{subsection.3.3}}
\newlabel{sec:models_snli}{{3.3}{16}{Neural Models for NLI}{subsection.3.3}{}}
\acronymused{SNLI}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.1}Sentence Encoding Models}{16}{subsubsection.3.3.1}}
\acronymused{MLP}
\acronymused{biLSTM}
\acronymused{SNLI}
\citation{parikh2016decomposable}
\citation{chen2017enhanced}
\citation{parikh2016decomposable}
\citation{mou2015natural}
\citation{chen2017enhanced}
\citation{chen2017natural}
\citation{chen2017natural}
\citation{chen2017natural}
\citation{tay2017compare,peters2018deep,ghaeini2018dr}
\citation{im2017distance}
\citation{gong2017natural}
\citation{hu2016deep}
\citation{xu2014rc}
\citation{liu2015learning}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.2}Inter-Sentence-Attention-based Models}{17}{subsubsection.3.3.2}}
\acronymused{NLI}
\acronymused{SNLI}
\acronymused{BoW}
\acronymused{LSTM}
\AC@undonewlabel{acro:ESIM}
\newlabel{acro:ESIM}{{3.3.2}{17}{}{section*.20}{}}
\acronymused{ESIM}
\acronymused{biLSTM}
\acronymused{biLSTM}
\acronymused{SNLI}
\AC@undonewlabel{acro:KIM}
\newlabel{acro:KIM}{{3.3.2}{17}{}{section*.21}{}}
\acronymused{KIM}
\acronymused{SNLI}
\acronymused{SNLI}
\acronymused{SNLI}
\acronymused{SNLI}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Integration of external Resources into Neural Networks}{17}{subsection.3.4}}
\newlabel{sec:ext_res_in_nn}{{3.4}{17}{Integration of external Resources into Neural Networks}{subsection.3.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.1}Improving word-embeddings}{17}{subsubsection.3.4.1}}
\citation{faruqui2015retrofitting}
\citation{mrkvsic2017semantic}
\citation{vulic2017specialising}
\citation{levy2015improving}
\citation{ruckle2018concatenated}
\acronymused{NLI}
\acronymused{SNLI}
\citation{goldberg2017Apr}
\citation{shen2018reinforced}
\citation{im2017distance}
\citation{goldberg2017Apr}
\citation{goldberg2017Apr}
\citation{nie2017shortcut}
\@writefile{toc}{\contentsline {section}{\numberline {4}Understanding Shortcut-Stacked-Encoder}{19}{section.4}}
\newlabel{sec:understanding}{{4}{19}{Understanding Shortcut-Stacked-Encoder}{section.4}{}}
\acronymused{SNLI}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Motivation}{19}{subsection.4.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Insights on the sentence representation}{19}{subsection.4.2}}
\newlabel{sec:insights_sent_repr}{{4.2}{19}{Insights on the sentence representation}{subsection.4.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.1}Approach}{19}{subsubsection.4.2.1}}
\acronymused{SNLI}
\newlabel{sec:understanding1_method}{{4.2.1}{19}{}{subsubsection.4.2.1}{}}
\acronymused{LSTM}
\acronymused{RNN}
\acronymused{RNN}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces General architecture of a \ac {RNN} (left). Example sentence in an unrolled \ac {RNN} (right).}}{19}{figure.4}}
\acronymused{RNN}
\acronymused{RNN}
\newlabel{fig:rnn}{{4}{19}{General architecture of a \ac {RNN} (left). Example sentence in an unrolled \ac {RNN} (right)}{figure.4}{}}
\acronymused{LSTM}
\acronymused{RNN}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Visualized example of extracting interpretable information of the max-pooled seentence representations with a dimensionality of 3.}}{20}{figure.5}}
\newlabel{fig:example_process_understanding}{{5}{20}{Visualized example of extracting interpretable information of the max-pooled seentence representations with a dimensionality of 3}{figure.5}{}}
\acronymused{biLSTM}
\acronymused{SNLI}
\acronymused{POS}
\acronymused{POS}
\acronymused{POS}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.2}Detection of relevant dimensions}{20}{subsubsection.4.2.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.3}Dimension-wise Analysis}{21}{subsubsection.4.2.3}}
\@writefile{toc}{\contentsline {paragraph}{\numberline {4.2.3.1}Positional information}{21}{paragraph.4.2.3.1}}
\@writefile{toc}{\contentsline {paragraph}{\numberline {4.2.3.2}Semantic information}{21}{paragraph.4.2.3.2}}
\@writefile{toc}{\contentsline {paragraph}{\numberline {4.2.3.3}Syntactic information}{21}{paragraph.4.2.3.3}}
\@writefile{toc}{\contentsline {paragraph}{\numberline {4.2.3.4}Evaluation of the impact of female and male dimensions}{21}{paragraph.4.2.3.4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.4}Conclusion}{21}{subsubsection.4.2.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Insights on the sentence alignment}{21}{subsection.4.3}}
\newlabel{sec:insights_sent_alignment}{{4.3}{21}{Insights on the sentence alignment}{subsection.4.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.1}Approach}{21}{subsubsection.4.3.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.2}Entailment analysis}{21}{subsubsection.4.3.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.3}Neutral and contradiction analysis}{21}{subsubsection.4.3.3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.4}Experiments}{21}{subsubsection.4.3.4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.5}Conclusion}{21}{subsubsection.4.3.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Errors of the base model}{21}{subsection.4.4}}
\citation{maccartney2007natural}
\citation{cooper1996role}
\@writefile{toc}{\contentsline {section}{\numberline {5}Additional SNLI test-set}{22}{section.5}}
\newlabel{sec:additional_snli_set}{{5}{22}{Additional SNLI test-set}{section.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Motivation}{22}{subsection.5.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Dataset}{22}{subsection.5.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.2.1}Creation}{22}{subsubsection.5.2.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.2.2}Validation}{22}{subsubsection.5.2.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.2.3}Final dataset}{22}{subsubsection.5.2.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Other models}{22}{subsection.5.3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.3.1}ESIM}{22}{subsubsection.5.3.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.3.2}Decomposable Attention}{22}{subsubsection.5.3.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4}Evaluation}{22}{subsection.5.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5}Analysis}{22}{subsection.5.5}}
\citation{rubinstein2015well}
\citation{levy2015improving}
\@writefile{toc}{\contentsline {section}{\numberline {6}Approaches to incorporate WordNet information}{23}{section.6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Extraction of WordNet data}{23}{subsection.6.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Integrating information into word-embeddings}{23}{subsection.6.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.2.1}Motivation}{23}{subsubsection.6.2.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.2.2}Concatenating pre-trained word-embeddings}{23}{subsubsection.6.2.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.2.3}Concatenation categorical information}{23}{subsubsection.6.2.3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.2.4}Analysis}{23}{subsubsection.6.2.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3}Multitask Learning}{23}{subsection.6.3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.3.1}Motivation}{23}{subsubsection.6.3.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.3.2}Architecture}{23}{subsubsection.6.3.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.3.3}Approaches}{23}{subsubsection.6.3.3}}
\@writefile{toc}{\contentsline {paragraph}{\numberline {6.3.3.1}Different sizes of multitask MLP}{23}{paragraph.6.3.3.1}}
\@writefile{toc}{\contentsline {paragraph}{\numberline {6.3.3.2}Introducing Dropout}{23}{paragraph.6.3.3.2}}
\@writefile{toc}{\contentsline {paragraph}{\numberline {6.3.3.3}Introducing an additional shared layer}{23}{paragraph.6.3.3.3}}
\@writefile{toc}{\contentsline {paragraph}{\numberline {6.3.3.4}Fixing multitasking network during training}{23}{paragraph.6.3.3.4}}
\@writefile{toc}{\contentsline {paragraph}{\numberline {6.3.3.5}Focusing on original words within sentence representation}{23}{paragraph.6.3.3.5}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.3.4}Analysis}{23}{subsubsection.6.3.4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.3.5}Evaluation}{23}{subsubsection.6.3.5}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Conclusion}{24}{section.7}}
\bibdata{bib/literature}
\bibcite{balazs2017refining}{{1}{2017}{{Balazs et~al.}}{{Balazs, Marrese-Taylor, Loyola, and Matsuo}}}
\bibcite{bengio2013representation}{{2}{2013}{{Bengio et~al.}}{{Bengio, Courville, and Vincent}}}
\bibcite{bos2005recognising}{{3}{2005}{{Bos and Markert}}{{}}}
\bibcite{bowman2015large}{{4}{2015}{{Bowman et~al.}}{{Bowman, Angeli, Potts, and Manning}}}
\bibcite{bowman2016fast}{{5}{2016}{{Bowman et~al.}}{{Bowman, Gauthier, Rastogi, Gupta, Manning, and Potts}}}
\bibcite{bromley1994signature}{{6}{1994}{{Bromley et~al.}}{{Bromley, Guyon, LeCun, S{\"a}ckinger, and Shah}}}
\bibcite{celikyilmaz2010enriching}{{7}{2010}{{Celikyilmaz et~al.}}{{Celikyilmaz, Hakkani-Tur, Pasupat, and Sarikaya}}}
\bibcite{chatzikyriakidis2017overview}{{8}{2017}{{Chatzikyriakidis et~al.}}{{Chatzikyriakidis, Cooper, Dobnik, and Larsson}}}
\bibcite{chen2017natural}{{9}{2017{a}}{{Chen et~al.}}{{Chen, Zhu, Ling, and Inkpen}}}
\bibcite{chen2017enhanced}{{10}{2017{b}}{{Chen et~al.}}{{Chen, Zhu, Ling, Wei, Jiang, and Inkpen}}}
\bibcite{chen2017recurrent}{{11}{2017{c}}{{Chen et~al.}}{{Chen, Zhu, Ling, Wei, Jiang, and Inkpen}}}
\bibcite{cheng2016long}{{12}{2016}{{Cheng et~al.}}{{Cheng, Dong, and Lapata}}}
\bibcite{clark2016combining}{{13}{2016}{{Clark et~al.}}{{Clark, Etzioni, Khot, Sabharwal, Tafjord, Turney, and Khashabi}}}
\bibcite{cooper1996using}{{14}{1996}{{Cooper et~al.}}{{Cooper, Crouch, Van~Eijck, Fox, Van~Genabith, Jaspars, Kamp, Milward, Pinkal, Poesio et~al.}}}
\bibcite{dagan2006pascal}{{15}{2006}{{Dagan et~al.}}{{Dagan, Glickman, and Magnini}}}
\bibcite{faruqui2015retrofitting}{{16}{2015}{{Faruqui et~al.}}{{Faruqui, Dodge, Jauhar, Dyer, Hovy, and Smith}}}
\bibcite{ghaeini2018dr}{{17}{2018}{{Ghaeini et~al.}}{{Ghaeini, Hasan, Datla, Liu, Lee, Qadir, Ling, Prakash, Fern, and Farri}}}
\bibcite{goldberg2017Apr}{{18}{2017}{{Goldberg}}{{}}}
\bibcite{gong2017natural}{{19}{2017}{{Gong et~al.}}{{Gong, Luo, and Zhang}}}
\bibcite{graves2005framewise}{{20}{2005}{{Graves and Schmidhuber}}{{}}}
\bibcite{gurevych2012uby}{{21}{2012}{{Gurevych et~al.}}{{Gurevych, Eckle-Kohler, Hartmann, Matuschek, Meyer, and Wirth}}}
\bibcite{gurevych2016linked}{{22}{2016}{{Gurevych et~al.}}{{Gurevych, Eckle-Kohler, and Matuschek}}}
\bibcite{gururangan2018annotation}{{23}{2018}{{Gururangan et~al.}}{{Gururangan, Swayamdipta, Levy, Schwartz, Bowman, and Smith}}}
\bibcite{hochreiter1997long}{{24}{1997}{{Hochreiter and Schmidhuber}}{{}}}
\bibcite{hu2016deep}{{25}{2016}{{Hu et~al.}}{{Hu, Yang, Salakhutdinov, and Xing}}}
\bibcite{ide2001american}{{26}{2001}{{Ide and Macleod}}{{}}}
\bibcite{ide2004american}{{27}{2004}{{Ide and Suderman}}{{}}}
\bibcite{ide2006integrating}{{28}{2006}{{Ide and Suderman}}{{}}}
\bibcite{im2017distance}{{29}{2017}{{Im and Cho}}{{}}}
\bibcite{Jurafsky2008May}{{30}{2008}{{Jurafsky and Martin}}{{}}}
\bibcite{scitail}{{31}{2018}{{Khot et~al.}}{{Khot, Sabharwal, and Clark}}}
\bibcite{kingma2014adam}{{32}{2014}{{Kingma and Ba}}{{}}}
\bibcite{levy2015improving}{{33}{2015}{{Levy et~al.}}{{Levy, Goldberg, and Dagan}}}
\bibcite{liu2015learning}{{34}{2015}{{Liu et~al.}}{{Liu, Jiang, Wei, Ling, and Hu}}}
\bibcite{maccartney2008phrase}{{35}{2008}{{MacCartney et~al.}}{{MacCartney, Galley, and Manning}}}
\bibcite{maccartney2007natural}{{36}{2007}{{MacCartney and Manning}}{{}}}
\bibcite{marelli2014semeval}{{37}{2014}{{Marelli et~al.}}{{Marelli, Bentivogli, Baroni, Bernardi, Menini, and Zamparelli}}}
\bibcite{mccarthy2004using}{{38}{2004}{{McCarthy et~al.}}{{McCarthy, Koeling, Weeds, and Carroll}}}
\bibcite{mikolov2013distributed}{{39}{2013}{{Mikolov et~al.}}{{Mikolov, Sutskever, Chen, Corrado, and Dean}}}
\bibcite{miller1995wordnet}{{40}{1995}{{Miller}}{{}}}
\bibcite{mou2015natural}{{41}{2015}{{Mou et~al.}}{{Mou, Men, Li, Xu, Zhang, Yan, and Jin}}}
\bibcite{mrkvsic2017semantic}{{42}{2017}{{Mrk{\v {s}}i{\'c} et~al.}}{{Mrk{\v {s}}i{\'c}, Vuli{\'c}, S{\'e}aghdha, Leviant, Reichart, Ga{\v {s}}i{\'c}, Korhonen, and Young}}}
\bibcite{munkhdalai2017neural}{{43}{2017}{{Munkhdalai and Yu}}{{}}}
\bibcite{murphy2003semantic}{{44}{2003}{{Murphy}}{{}}}
\bibcite{nangia2017repeval}{{45}{2017}{{Nangia et~al.}}{{Nangia, Williams, Lazaridou, and Bowman}}}
\bibcite{nie2017shortcut}{{46}{2017}{{Nie and Bansal}}{{}}}
\bibcite{parikh2016decomposable}{{47}{2016}{{Parikh et~al.}}{{Parikh, T{\"a}ckstr{\"o}m, Das, and Uszkoreit}}}
\bibcite{pennington2014glove}{{48}{2014}{{Pennington et~al.}}{{Pennington, Socher, and Manning}}}
\bibcite{peters2018deep}{{49}{2018}{{Peters et~al.}}{{Peters, Neumann, Iyyer, Gardner, Clark, Lee, and Zettlemoyer}}}
\bibcite{prakash2007learning}{{50}{2007}{{Prakash et~al.}}{{Prakash, Jurafsky, and Ng}}}
\bibcite{resnik1995using}{{51}{1995}{{Resnik}}{{}}}
\bibcite{rocktaschel2015reasoning}{{52}{2015}{{Rockt{\"a}schel et~al.}}{{Rockt{\"a}schel, Grefenstette, Hermann, Ko{\v {c}}isk{\`y}, and Blunsom}}}
\bibcite{rubinstein2015well}{{53}{2015}{{Rubinstein et~al.}}{{Rubinstein, Levi, Schwartz, and Rappoport}}}
\bibcite{ruckle2018concatenated}{{54}{2018}{{R{\"u}ckl{\'e} et~al.}}{{R{\"u}ckl{\'e}, Eger, Peyrard, and Gurevych}}}
\bibcite{shen2018reinforced}{{55}{2018}{{Shen et~al.}}{{Shen, Zhou, Long, Jiang, Wang, and Zhang}}}
\bibcite{suchanek2007yago}{{56}{2007}{{Suchanek et~al.}}{{Suchanek, Kasneci, and Weikum}}}
\bibcite{tatu2005semantic}{{57}{2005}{{Tatu and Moldovan}}{{}}}
\bibcite{tay2017compare}{{58}{2017}{{Tay et~al.}}{{Tay, Tuan, and Hui}}}
\bibcite{vulic2017specialising}{{59}{2017}{{Vuli{\'c} and Mrk{\v {s}}i{\'c}}}{{}}}
\bibcite{vulic2017morph}{{60}{2017}{{Vuli{\'c} et~al.}}{{Vuli{\'c}, Mrk{\v {s}}i{\'c}, Reichart, S{\'e}aghdha, Young, and Korhonen}}}
\bibcite{welbl2017crowdsourcing}{{61}{2017}{{Welbl et~al.}}{{Welbl, Liu, and Gardner}}}
\bibcite{williams2017broad}{{62}{2017}{{Williams et~al.}}{{Williams, Nangia, and Bowman}}}
\bibcite{xu2014rc}{{63}{2014}{{Xu et~al.}}{{Xu, Bai, Bian, Gao, Wang, Liu, and Liu}}}
\bibcite{yang2017character}{{64}{2017}{{Yang et~al.}}{{Yang, Costa-juss{\`a}, and Fonollosa}}}
\bibcite{young2014image}{{65}{2014}{{Young et~al.}}{{Young, Lai, Hodosh, and Hockenmaier}}}
\bibcite{zesch2008extracting}{{66}{2008}{{Zesch et~al.}}{{Zesch, M{\"u}ller, and Gurevych}}}
\citation{nie2017shortcut}
\acronymused{RNN}
\acronymused{RNN}
\citation{nie2017shortcut}
\acronymused{SNLI}
\acronymused{MultiNLI}
\global\@namedef{scr@dte@section@lastmaxnumwidth}{9.35994pt}
\global\@namedef{scr@dte@subsection@lastmaxnumwidth}{17.00493pt}
\global\@namedef{scr@dte@subsubsection@lastmaxnumwidth}{24.92792pt}
\global\@namedef{scr@dte@table@lastmaxnumwidth}{9.08194pt}
\global\@namedef{scr@dte@figure@lastmaxnumwidth}{9.08194pt}
