\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand*\new@tpo@label[2]{}
\bibstyle{acl_natbib}
\AC@reset@newl@bel
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\babel@aux{english}{}
\newacro{biLSTM}[\AC@hyperlink{biLSTM}{biLSTM}]{bidirectional Long-Short-Term Memory Network}
\newacro{BoW}[\AC@hyperlink{BoW}{BoW}]{Bag of Words}
\newacro{ESIM}[\AC@hyperlink{ESIM}{ESIM}]{Enhanced Sequential Inference Model}
\newacro{HIT}[\AC@hyperlink{HIT}{HIT}]{Human Intelligence Task}
\newacro{IE}[\AC@hyperlink{IE}{IE}]{Information Extraction}
\newacro{IR}[\AC@hyperlink{IR}{IR}]{Information Retrieval}
\newacro{KIM}[\AC@hyperlink{KIM}{KIM}]{Knowledge-based Inference Model}
\newacro{LSTM}[\AC@hyperlink{LSTM}{LSTM}]{Long-Short-Term-Memory}
\newacro{MLP}[\AC@hyperlink{MLP}{MLP}]{Multi Layer Perceptron}
\newacro{MultiNLI}[\AC@hyperlink{MultiNLI}{MultiNLI}]{MultiGenre Natural Language Inference Corpus}
\newacro{NLI}[\AC@hyperlink{NLI}{NLI}]{Natural Language Inference}
\newacro{NLP}[\AC@hyperlink{NLP}{NLP}]{Natural Language Processing}
\newacro{NLU}[\AC@hyperlink{NLU}{NLU}]{Natural Language Understanding}
\newacro{POS}[\AC@hyperlink{POS}{POS}]{Part of Speech}
\newacro{OANC}[\AC@hyperlink{OANC}{OANC}]{Open American National Corpus}
\newacro{QA}[\AC@hyperlink{QA}{QA}]{Question Answering}
\newacro{RNN}[\AC@hyperlink{RNN}{RNN}]{Recurrent Neural Network}
\newacro{RTE}[\AC@hyperlink{RTE}{RTE}]{Recognizing Textual Entailment}
\newacro{SD}[\AC@hyperlink{SD}{SD}]{Standard Deviation}
\newacro{SICK}[\AC@hyperlink{SICK}{SICK}]{Sentences Involving Compositional Knowledge}
\newacro{SNLI}[\AC@hyperlink{SNLI}{SNLI}]{The Stanford Natural Language Inference Corpus}
\newacro{WSD}[\AC@hyperlink{WSD}{WSD}]{Word Sense Disambiguation}
\newacro{YAGO}[\AC@hyperlink{YAGO}{YAGO}]{Yet Another Great Ontology}
\citation{bengio2013representation}
\citation{mikolov2013distributed}
\citation{pennington2014glove}
\citation{celikyilmaz2010enriching}
\citation{vulic2017morph}
\citation{bowman2015large}
\citation{dagan2006pascal}
\citation{maccartney2007natural}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{5}{section.1}}
\AC@undonewlabel{acro:NLP}
\newlabel{acro:NLP}{{1}{5}{Introduction}{section*.1}{}}
\acronymused{NLP}
\AC@undonewlabel{acro:NLU}
\newlabel{acro:NLU}{{1}{5}{Introduction}{section*.2}{}}
\acronymused{NLU}
\acronymused{NLU}
\acronymused{NLP}
\AC@undonewlabel{acro:NLI}
\newlabel{acro:NLI}{{1}{5}{Introduction}{section*.3}{}}
\acronymused{NLI}
\AC@undonewlabel{acro:RTE}
\newlabel{acro:RTE}{{1}{5}{Introduction}{section*.4}{}}
\acronymused{RTE}
\acronymused{NLU}
\acronymused{NLP}
\AC@undonewlabel{acro:LSTM}
\newlabel{acro:LSTM}{{1}{5}{}{section*.5}{}}
\acronymused{LSTM}
\AC@undonewlabel{acro:RNN}
\newlabel{acro:RNN}{{1}{5}{}{section*.6}{}}
\acronymused{RNN}
\acronymused{NLP}
\acronymused{NLI}
\acronymused{NLI}
\acronymused{NLU}
\acronymused{NLI}
\citation{bowman2015large}
\citation{dagan2009recognizing}
\citation{maccartney2008phrase}
\citation{maccartney2007natural,bos2005recognising}
\citation{dagan2009recognizing}
\citation{williams2017broad,cooper1996using,bos2005recognising,dagan2006pascal}
\citation{shwartz2015learning}
\citation{murphy2003semantic}
\citation{dagan2009recognizing}
\citation{Jurafsky2008May}
\citation{Jurafsky2008May}
\@writefile{toc}{\contentsline {section}{\numberline {2}Theoretical Background}{6}{section.2}}
\newlabel{sec:basics}{{2}{6}{Theoretical Background}{section.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Natural Language Inference}{6}{subsection.2.1}}
\newlabel{sec:basics_nli}{{2.1}{6}{Natural Language Inference}{subsection.2.1}{}}
\acronymused{NLI}
\acronymused{NLI}
\acronymused{NLI}
\acronymused{NLI}
\acronymused{NLP}
\acronymused{NLU}
\acronymused{NLP}
\AC@undonewlabel{acro:QA}
\newlabel{acro:QA}{{2.1}{6}{}{section*.7}{}}
\acronymused{QA}
\AC@undonewlabel{acro:IE}
\newlabel{acro:IE}{{2.1}{6}{}{section*.8}{}}
\acronymused{IE}
\acronymused{QA}
\acronymused{IE}
\acronymused{NLP}
\acronymused{NLU}
\acronymused{NLU}
\acronymused{NLI}
\acronymused{NLU}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Lexical Semantic Relations}{6}{subsection.2.2}}
\newlabel{sec:word_relations}{{2.2}{6}{Lexical Semantic Relations}{subsection.2.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.1}Synonymy and antonomy}{6}{subsubsection.2.2.1}}
\citation{Jurafsky2008May}
\citation{Jurafsky2008May}
\citation{nie2017shortcut}
\citation{bromley1994signature}
\citation{nie2017shortcut}
\citation{nie2017shortcut}
\citation{hochreiter1997long}
\citation{graves2005framewise}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces A sample ontology of animals to illustrate the lexical relations \textit  {Hypernomy} and \textit  {Holonymy}.}}{7}{figure.1}}
\newlabel{fig:lexical_resources}{{1}{7}{A sample ontology of animals to illustrate the lexical relations \textit {Hypernomy} and \textit {Holonymy}}{figure.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.2}Hypernomy}{7}{subsubsection.2.2.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.3}Holonomy}{7}{subsubsection.2.2.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Shortcut-Stacked-Encoder and Residual Encoder}{7}{subsection.2.3}}
\newlabel{sec:residual_encoder_def}{{2.3}{7}{Shortcut-Stacked-Encoder and Residual Encoder}{subsection.2.3}{}}
\acronymused{NLI}
\acronymused{NLI}
\AC@undonewlabel{acro:MLP}
\newlabel{acro:MLP}{{2.3}{7}{Shortcut-Stacked-Encoder and Residual Encoder}{section*.9}{}}
\acronymused{MLP}
\citation{nie2017shortcut}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.1}Sentence Encoding for Shortcut-Stacked-Encoder}{8}{subsubsection.2.3.1}}
\AC@undonewlabel{acro:biLSTM}
\newlabel{acro:biLSTM}{{2.3.1}{8}{Sentence Encoding for Shortcut-Stacked-Encoder}{section*.10}{}}
\acronymused{biLSTM}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces The architecture of the sentence-encoding component within the Shortcut-Stacked-Encoder, taken from \cite  {nie2017shortcut}.}}{8}{figure.2}}
\newlabel{fig:sentence_emcoder_shortcut}{{2}{8}{The architecture of the sentence-encoding component within the Shortcut-Stacked-Encoder, taken from \cite {nie2017shortcut}}{figure.2}{}}
\acronymused{LSTM}
\acronymused{biLSTM}
\acronymused{biLSTM}
\acronymused{biLSTM}
\acronymused{biLSTM}
\acronymused{biLSTM}
\acronymused{biLSTM}
\newlabel{eq:stacked_encoder_input}{{2}{8}{Sentence Encoding for Shortcut-Stacked-Encoder}{equation.2.2}{}}
\acronymused{biLSTM}
\acronymused{biLSTM}
\citation{mou2015natural}
\citation{nie2017shortcut}
\citation{kingma2014adam}
\citation{pennington2014glove}
\citation{nie2017shortcut}
\citation{gong2017natural}
\citation{gong2017natural}
\citation{nie2017shortcut}
\citation{nie2017shortcut}
\citation{nie2017shortcut}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.2}Classification}{9}{subsubsection.2.3.2}}
\acronymused{MLP}
\acronymused{MLP}
\acronymused{NLI}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.3}Training}{9}{subsubsection.2.3.3}}
\acronymused{MLP}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.4}Residual Encoder and Reimplementation Variants}{9}{subsubsection.2.3.4}}
\acronymused{biLSTM}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Accuracy in percent of different implementations of the model from \cite  {nie2017shortcut}, achieved on the SNLI dataset compared with human performance.}}{9}{table.1}}
\newlabel{table:reimplementation_performance}{{1}{9}{Accuracy in percent of different implementations of the model from \cite {nie2017shortcut}, achieved on the SNLI dataset compared with human performance}{table.1}{}}
\acronymused{biLSTM}
\acronymused{MLP}
\acronymused{NLI}
\acronymused{biLSTM}
\acronymused{MLP}
\citation{bos2005recognising,tatu2005semantic}
\citation{miller1995wordnet}
\citation{Jurafsky2008May}
\citation{Jurafsky2008May}
\citation{mccarthy2004using}
\citation{resnik1995using}
\citation{prakash2007learning}
\citation{gurevych2016linked}
\citation{zesch2008extracting}
\citation{gurevych2016linked}
\@writefile{toc}{\contentsline {section}{\numberline {3}Related Work}{11}{section.3}}
\newlabel{sec:related_work}{{3}{11}{Related Work}{section.3}{}}
\acronymused{NLI}
\acronymused{NLP}
\acronymused{NLI}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}External Resources}{11}{subsection.3.1}}
\newlabel{sec:ext_resources}{{3.1}{11}{External Resources}{subsection.3.1}{}}
\acronymused{NLI}
\acronymused{NLP}
\acronymused{NLI}
\acronymused{NLU}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.1}WordNet}{11}{subsubsection.3.1.1}}
\newlabel{sec:wordnet}{{3.1.1}{11}{WordNet}{subsubsection.3.1.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Example of different synsets of the lemma ``table'' (only noun senses) within WordNet, taken from \href  {http://wordnetweb.princeton.edu}{http://wordnetweb.princeton.edu}.}}{11}{figure.3}}
\newlabel{fig:wordnet}{{3}{11}{Example of different synsets of the lemma ``table'' (only noun senses) within WordNet, taken from \href {http://wordnetweb.princeton.edu}{http://wordnetweb.princeton.edu}}{figure.3}{}}
\AC@undonewlabel{acro:POS}
\newlabel{acro:POS}{{3.1.1}{11}{WordNet}{section*.11}{}}
\acronymused{POS}
\AC@undonewlabel{acro:WSD}
\newlabel{acro:WSD}{{3.1.1}{11}{}{section*.12}{}}
\acronymused{WSD}
\citation{suchanek2007yago}
\citation{gurevych2012uby}
\citation{cooper1996using}
\citation{dagan2006pascal}
\citation{marelli2014semeval}
\citation{young2014image}
\citation{bowman2015large}
\citation{bowman2015large}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.2}Wikipedia}{12}{subsubsection.3.1.2}}
\newlabel{sec:wikipedia}{{3.1.2}{12}{Wikipedia}{subsubsection.3.1.2}{}}
\acronymused{NLP}
\acronymused{NLI}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.3}Derived from multiple Knowledge Bases}{12}{subsubsection.3.1.3}}
\AC@undonewlabel{acro:YAGO}
\newlabel{acro:YAGO}{{3.1.3}{12}{Derived from multiple Knowledge Bases}{section*.13}{}}
\acronymused{YAGO}
\acronymused{YAGO}
\acronymused{NLP}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Datasets for NLI}{12}{subsection.3.2}}
\newlabel{sec:basics_datasets}{{3.2}{12}{Datasets for NLI}{subsection.3.2}{}}
\acronymused{NLI}
\AC@undonewlabel{acro:SNLI}
\newlabel{acro:SNLI}{{3.2}{12}{Datasets for NLI}{section*.14}{}}
\acronymused{SNLI}
\acronymused{NLI}
\AC@undonewlabel{acro:SICK}
\newlabel{acro:SICK}{{3.2}{12}{Datasets for NLI}{section*.15}{}}
\acronymused{SICK}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1}SNLI}{12}{subsubsection.3.2.1}}
\newlabel{sec:snli}{{3.2.1}{12}{SNLI}{subsubsection.3.2.1}{}}
\acronymused{SNLI}
\acronymused{NLI}
\acronymused{NLI}
\citation{bowman2015large}
\citation{young2014image}
\citation{gururangan2018annotation}
\acronymused{SNLI}
\acronymused{SNLI}
\acronymused{SNLI}
\acronymused{SNLI}
\acronymused{SNLI}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Example sentence pairs, taken from \ac {SNLI}, showing typical sentences within the dataset.}}{13}{table.2}}
\acronymused{SNLI}
\newlabel{table:snli_example}{{2}{13}{Example sentence pairs, taken from \ac {SNLI}, showing typical sentences within the dataset}{table.2}{}}
\acronymused{SNLI}
\citation{chatzikyriakidis2017overview,williams2017broad}
\citation{williams2017broad}
\citation{bowman2015large}
\citation{ide2001american,ide2004american,ide2006integrating}
\citation{nangia2017repeval}
\citation{williams2017broad}
\citation{chen2017recurrent}
\citation{nie2017shortcut}
\citation{nie2017shortcut,balazs2017refining,yang2017character}
\citation{scitail}
\AC@undonewlabel{acro:BoW}
\newlabel{acro:BoW}{{3.2.1}{14}{}{section*.16}{}}
\acronymused{BoW}
\acronymused{SNLI}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.2}MultiNLI}{14}{subsubsection.3.2.2}}
\acronymused{SNLI}
\acronymused{SNLI}
\AC@undonewlabel{acro:MultiNLI}
\newlabel{acro:MultiNLI}{{3.2.2}{14}{MultiNLI}{section*.17}{}}
\acronymused{MultiNLI}
\acronymused{MultiNLI}
\AC@undonewlabel{acro:OANC}
\newlabel{acro:OANC}{{3.2.2}{14}{}{section*.18}{}}
\acronymused{OANC}
\acronymused{MultiNLI}
\acronymused{NLI}
\acronymused{NLU}
\acronymused{SNLI}
\acronymused{NLU}
\acronymused{MultiNLI}
\acronymused{SNLI}
\acronymused{SNLI}
\acronymused{MultiNLI}
\acronymused{SNLI}
\acronymused{MultiNLI}
\acronymused{OANC}
\citation{scitail}
\citation{welbl2017crowdsourcing}
\citation{scitail}
\citation{clark2016combining}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Example sentence pairs from \ac {MultiNLI}, taken from RepEval 2017 Shared Task, showing samples of different genres.}}{15}{table.3}}
\acronymused{MultiNLI}
\newlabel{table:multinli_example}{{3}{15}{Example sentence pairs from \ac {MultiNLI}, taken from RepEval 2017 Shared Task, showing samples of different genres}{table.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.3}SciTail}{15}{subsubsection.3.2.3}}
\acronymused{NLI}
\acronymused{QA}
\acronymused{SNLI}
\acronymused{MultiNLI}
\acronymused{NLI}
\acronymused{QA}
\AC@undonewlabel{acro:IR}
\newlabel{acro:IR}{{2}{15}{}{section*.19}{}}
\acronymused{IR}
\acronymused{SNLI}
\acronymused{MultiNLI}
\acronymused{QA}
\acronymused{NLI}
\acronymused{QA}
\acronymused{NLU}
\citation{bromley1994signature}
\citation{bowman2016fast}
\citation{bowman2016fast}
\citation{munkhdalai2017neural}
\citation{chen2017recurrent}
\citation{nie2017shortcut}
\citation{shen2018reinforced,im2017distance}
\citation{shen2018reinforced}
\citation{shen2018reinforced}
\citation{shen2018reinforced}
\citation{im2017distance}
\citation{rocktaschel2015reasoning}
\citation{cheng2016long}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Example sentence pairs from SciTail Task, different premises retrieved for two hypothesis.}}{16}{table.4}}
\newlabel{table:scitail_example}{{4}{16}{Example sentence pairs from SciTail Task, different premises retrieved for two hypothesis}{table.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Neural Models for NLI}{16}{subsection.3.3}}
\newlabel{sec:models_snli}{{3.3}{16}{Neural Models for NLI}{subsection.3.3}{}}
\acronymused{SNLI}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.1}Sentence Encoding Models}{16}{subsubsection.3.3.1}}
\acronymused{MLP}
\acronymused{biLSTM}
\acronymused{SNLI}
\citation{parikh2016decomposable}
\citation{chen2017enhanced}
\citation{parikh2016decomposable}
\citation{mou2015natural}
\citation{chen2017enhanced}
\citation{chen2017natural}
\citation{chen2017natural}
\citation{chen2017natural}
\citation{tay2017compare,peters2018deep,ghaeini2018dr}
\citation{im2017distance}
\citation{gong2017natural}
\citation{hu2016deep}
\citation{xu2014rc}
\citation{liu2015learning}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.2}Inter-Sentence-Attention-based Models}{17}{subsubsection.3.3.2}}
\newlabel{sec:rel_work_sentence_encoding_models}{{3.3.2}{17}{Inter-Sentence-Attention-based Models}{subsubsection.3.3.2}{}}
\acronymused{NLI}
\acronymused{SNLI}
\acronymused{BoW}
\acronymused{LSTM}
\AC@undonewlabel{acro:ESIM}
\newlabel{acro:ESIM}{{3.3.2}{17}{}{section*.20}{}}
\acronymused{ESIM}
\acronymused{biLSTM}
\acronymused{biLSTM}
\acronymused{SNLI}
\AC@undonewlabel{acro:KIM}
\newlabel{acro:KIM}{{3.3.2}{17}{}{section*.21}{}}
\acronymused{KIM}
\acronymused{SNLI}
\acronymused{SNLI}
\acronymused{SNLI}
\acronymused{SNLI}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Integration of external Resources into Neural Networks}{17}{subsection.3.4}}
\newlabel{sec:ext_res_in_nn}{{3.4}{17}{Integration of external Resources into Neural Networks}{subsection.3.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.1}Improving word-embeddings}{17}{subsubsection.3.4.1}}
\newlabel{sec:embeddings_improvements_relwork}{{3.4.1}{17}{Improving word-embeddings}{subsubsection.3.4.1}{}}
\citation{faruqui2015retrofitting}
\citation{mrkvsic2017semantic}
\citation{vulic2017specialising}
\citation{levy2015improving}
\citation{ruckle2018concatenated}
\acronymused{NLI}
\acronymused{SNLI}
\citation{goldberg2017Apr}
\citation{shen2018reinforced}
\citation{im2017distance}
\citation{goldberg2017Apr}
\citation{goldberg2017Apr}
\citation{nie2017shortcut}
\@writefile{toc}{\contentsline {section}{\numberline {4}Understanding Shortcut-Stacked-Encoder}{19}{section.4}}
\newlabel{sec:understanding}{{4}{19}{Understanding Shortcut-Stacked-Encoder}{section.4}{}}
\acronymused{SNLI}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Motivation}{19}{subsection.4.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Insights on the sentence representation}{19}{subsection.4.2}}
\newlabel{sec:insights_sent_repr}{{4.2}{19}{Insights on the sentence representation}{subsection.4.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.1}Approach}{19}{subsubsection.4.2.1}}
\acronymused{SNLI}
\newlabel{sec:understanding1_method}{{4.2.1}{19}{}{subsubsection.4.2.1}{}}
\acronymused{LSTM}
\acronymused{RNN}
\acronymused{RNN}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces General architecture of a \ac {RNN} (left). Example sentence in an unrolled \ac {RNN} (right).}}{19}{figure.4}}
\acronymused{RNN}
\acronymused{RNN}
\newlabel{fig:rnn}{{4}{19}{General architecture of a \ac {RNN} (left). Example sentence in an unrolled \ac {RNN} (right)}{figure.4}{}}
\citation{Bishop2007}
\acronymused{LSTM}
\acronymused{RNN}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Visualized example of extracting interpretable information of the max-pooled seentence representations with a dimensionality of 3.}}{20}{figure.5}}
\newlabel{fig:example_process_understanding}{{5}{20}{Visualized example of extracting interpretable information of the max-pooled seentence representations with a dimensionality of 3}{figure.5}{}}
\acronymused{biLSTM}
\newlabel{sec:understanding1_analysed_data}{{4.2.1}{20}{}{figure.5}{}}
\acronymused{SNLI}
\acronymused{POS}
\acronymused{POS}
\acronymused{POS}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.2}Detection of relevant dimensions}{20}{subsubsection.4.2.2}}
\AC@undonewlabel{acro:SD}
\newlabel{acro:SD}{{4.2.2}{20}{Detection of relevant dimensions}{section*.22}{}}
\acronymused{SD}
\citation{dagan2000contextual}
\acronymused{SD}
\acronymused{SD}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces The standard deviation within a dimension of sentence representations (x-axis) by the amount of dimensions with the given standard deviation.}}{21}{figure.6}}
\newlabel{fig:sd}{{6}{21}{The standard deviation within a dimension of sentence representations (x-axis) by the amount of dimensions with the given standard deviation}{figure.6}{}}
\acronymused{SD}
\acronymused{SD}
\acronymused{NLI}
\acronymused{SD}
\acronymused{SD}
\acronymused{SD}
\citation{marcus1993building}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces An extraction of a grid-plot, showing dimensions with the position within the sentence of the word, responsible for the dimensional value.}}{22}{figure.7}}
\newlabel{fig:find_position_1}{{7}{22}{An extraction of a grid-plot, showing dimensions with the position within the sentence of the word, responsible for the dimensional value}{figure.7}{}}
\acronymused{SNLI}
\acronymused{POS}
\acronymused{POS}
\acronymused{POS}
\acronymused{SD}
\citation{gururangan2018annotation}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces An extraction of a grid-plot, showing syntatical information using the \ac {POS} tag with pre-sorted rows to have a single dominant label.}}{23}{figure.8}}
\acronymused{POS}
\newlabel{fig:find_syntax}{{8}{23}{An extraction of a grid-plot, showing syntatical information using the \ac {POS} tag with pre-sorted rows to have a single dominant label}{figure.8}{}}
\acronymused{SD}
\acronymused{SD}
\acronymused{SNLI}
\citation{mikolov2013linguistic}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces An extraction of a grid-plot, gender specific female using only sentences with words of pre-defined wordlists.}}{24}{figure.9}}
\newlabel{fig:find_male_female}{{9}{24}{An extraction of a grid-plot, gender specific female using only sentences with words of pre-defined wordlists}{figure.9}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.3}Female and male dimensions}{24}{subsubsection.4.2.3}}
\newlabel{sec:understanding2}{{4.2.3}{24}{Female and male dimensions}{subsubsection.4.2.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Representation visualitation with respect to genders of dimension 199 (left) and dimension 602 (right).}}{24}{figure.10}}
\newlabel{fig:mf_basic}{{10}{24}{Representation visualitation with respect to genders of dimension 199 (left) and dimension 602 (right)}{figure.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Detailed representation visualitation of different terms for human males of dimension 199 (left) and dimension 602 (right).}}{25}{figure.11}}
\newlabel{fig:mf_detailed_m}{{11}{25}{Detailed representation visualitation of different terms for human males of dimension 199 (left) and dimension 602 (right)}{figure.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Detailed representation visualitation of different terms for human females of dimension 845 (left) and dimension 311 (right).}}{25}{figure.12}}
\newlabel{fig:mf_detailed_f}{{12}{25}{Detailed representation visualitation of different terms for human females of dimension 845 (left) and dimension 311 (right)}{figure.12}{}}
\newlabel{tab:relevance_mf_nn}{{4.2.3}{26}{}{figure.12}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces Accuracies achieved on \ac {SNLI} using $|r|$-dimensional sentence representations of gender-specific dimensions.}}{26}{table.5}}
\acronymused{SNLI}
\acronymused{SNLI}
\acronymused{MLP}
\newlabel{eq:invert}{{8}{26}{}{equation.4.8}{}}
\acronymused{SNLI}
\newlabel{tab:inverted_mf_results_acc}{{4.2.3}{27}{}{equation.4.8}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces Results in terms of accuracy of inverted gender-specific dimensions on \ac {SNLI} train and dev set.}}{27}{table.6}}
\acronymused{SNLI}
\newlabel{tab:inverted_mf_results_samples}{{4.2.3}{27}{}{table.6}{}}
\@writefile{lot}{\contentsline {table}{\numberline {7}{\ignorespaces Comparison of samples between their predictions based on the original and gender-inverted sentence representations.}}{27}{table.7}}
\acronymused{SNLI}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.4}Other semantic dimensions}{28}{subsubsection.4.2.4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.5}Syntactic dimensions}{28}{subsubsection.4.2.5}}
\acronymused{POS}
\acronymused{POS}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Dimension 713 encoding verbs (left) and dimension 2020 encoding adjectives.}}{29}{figure.13}}
\newlabel{fig:find_syntax_vb}{{13}{29}{Dimension 713 encoding verbs (left) and dimension 2020 encoding adjectives}{figure.13}{}}
\acronymused{SNLI}
\acronymused{SNLI}
\acronymused{POS}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Dimension 757, encoding the subjects (left), and dimension 1840 encoding objects (right) of sentences.}}{30}{figure.14}}
\newlabel{fig:find_syntax_subj_obj}{{14}{30}{Dimension 757, encoding the subjects (left), and dimension 1840 encoding objects (right) of sentences}{figure.14}{}}
\acronymused{SNLI}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Insights on the sentence alignment}{30}{subsection.4.3}}
\newlabel{sec:insights_sent_alignment}{{4.3}{30}{Insights on the sentence alignment}{subsection.4.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.1}Alignment analysis on a single sample}{30}{subsubsection.4.3.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces Word alignments of an entailing sentence pair either by counting all shared dimensions (left) or only dimensions with at least a value of 0.2 (right).}}{31}{figure.15}}
\newlabel{fig:alignment_entailment_sample_general}{{15}{31}{Word alignments of an entailing sentence pair either by counting all shared dimensions (left) or only dimensions with at least a value of 0.2 (right)}{figure.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces Visualitation of an entailing sample with applied element-wise multiplication either using the mean (left) or maximum (right) product of all shared dimensions for each word pair.}}{31}{figure.16}}
\newlabel{fig:alignment_entailment_sample_mult}{{16}{31}{Visualitation of an entailing sample with applied element-wise multiplication either using the mean (left) or maximum (right) product of all shared dimensions for each word pair}{figure.16}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces Visualitation of a contradicting sample by counting meaningful shared dimensions (left) and meaningful distinct dimensions 8right) amongst pairs of words.}}{32}{figure.17}}
\newlabel{fig:alignment_contr_sample_general}{{17}{32}{Visualitation of a contradicting sample by counting meaningful shared dimensions (left) and meaningful distinct dimensions 8right) amongst pairs of words}{figure.17}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces Dimension-wise visualitation of distinct information represented by \textit  {sitting} in the premise and \textit  {standing} in the hypothesis.}}{33}{figure.18}}
\newlabel{fig:contradiction_alignment_unshared_dimwise}{{18}{33}{Dimension-wise visualitation of distinct information represented by \textit {sitting} in the premise and \textit {standing} in the hypothesis}{figure.18}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.2}Approach for a general alignment understanding}{33}{subsubsection.4.3.2}}
\newlabel{sec:approach_general_alignment_understanding}{{4.3.2}{33}{Approach for a general alignment understanding}{subsubsection.4.3.2}{}}
\acronymused{MLP}
\@writefile{lof}{\contentsline {figure}{\numberline {19}{\ignorespaces Visualitation of a sample sentence pair with explanatory guides for interpretation.}}{34}{figure.19}}
\newlabel{fig:sample_coverage_prob}{{19}{34}{Visualitation of a sample sentence pair with explanatory guides for interpretation}{figure.19}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.3}Entailment analysis}{34}{subsubsection.4.3.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {20}{\ignorespaces Visualitation of 150 sentence pairs ($p$, $h$), correctly labelled as entailment.}}{34}{figure.20}}
\newlabel{fig:entailment_uninversed}{{20}{34}{Visualitation of 150 sentence pairs ($p$, $h$), correctly labelled as entailment}{figure.20}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {21}{\ignorespaces Visualization of samples predicted as entailment (left) and neutral (right) after swapping $p$ and $h$.}}{35}{figure.21}}
\newlabel{fig:alignment_entailment_inversed}{{21}{35}{Visualization of samples predicted as entailment (left) and neutral (right) after swapping $p$ and $h$}{figure.21}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.4}Neutral and contradiction analysis}{35}{subsubsection.4.3.4}}
\citation{nie2017shortcut}
\@writefile{lof}{\contentsline {figure}{\numberline {22}{\ignorespaces Visualitazion of 150 sentence pairs ($p$, $h$) correctly labelled as \textit  {neutral} (left) and \textit  {contradiction} (right).}}{36}{figure.22}}
\newlabel{fig:neutr_contr_uninversed}{{22}{36}{Visualitazion of 150 sentence pairs ($p$, $h$) correctly labelled as \textit {neutral} (left) and \textit {contradiction} (right)}{figure.22}{}}
\acronymused{MLP}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.5}Summarizing the inisghts onmMax-pooled sentence representations}{36}{subsubsection.4.3.5}}
\acronymused{SNLI}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Identification of missing knowledge}{36}{subsection.4.4}}
\acronymused{SNLI}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.1}Approach}{37}{subsubsection.4.4.1}}
\acronymused{SNLI}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.2}Quantitative results}{37}{subsubsection.4.4.2}}
\newlabel{tab:misclassified_orig_contr}{{4.4.2}{37}{}{subsubsection.4.4.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {8}{\ignorespaces Misclassified samples with gold label \textit  {contradiction}, predicted as \textit  {entailment}.}}{37}{table.8}}
\acronymused{POS}
\acronymused{BoW}
\citation{pantel2005inducing}
\citation{pantel2005inducing}
\citation{lecun2015deep}
\citation{sahlgren2008distributional}
\newlabel{tab:misclassified_orig_ent}{{4.4.2}{38}{}{table.8}{}}
\@writefile{lot}{\contentsline {table}{\numberline {9}{\ignorespaces Misclassified samples with gold label \textit  {entailment}, predicted as \textit  {contradiction}.}}{38}{table.9}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.3}Conclusions}{38}{subsubsection.4.4.3}}
\acronymused{NLI}
\citation{glockner_acl18}
\citation{jia-liang:2017:EMNLP2017}
\citation{gururangan2018annotation}
\citation{dasgupta2018evaluating}
\@writefile{toc}{\contentsline {section}{\numberline {5}Additional SNLI test-set}{39}{section.5}}
\newlabel{sec:additional_snli_set}{{5}{39}{Additional SNLI test-set}{section.5}{}}
\newlabel{table:correct_samples}{{5}{39}{Additional SNLI test-set}{section.5}{}}
\@writefile{lot}{\contentsline {table}{\numberline {10}{\ignorespaces Correctly classified examples.}}{39}{table.10}}
\acronymused{NLI}
\acronymused{NLU}
\acronymused{NLU}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Goal of the new test-set}{39}{subsection.5.1}}
\acronymused{SNLI}
\acronymused{SNLI}
\acronymused{SNLI}
\acronymused{NLU}
\acronymused{NLU}
\acronymused{SNLI}
\acronymused{SNLI}
\citation{goldberg2017Apr}
\citation{bowman2015large}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Dataset}{40}{subsection.5.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.2.1}Creation of adversarial samples}{40}{subsubsection.5.2.1}}
\acronymused{SNLI}
\acronymused{RTE}
\newlabel{tab:new_testset_samples}{{5.2.1}{40}{Creation of adversarial samples}{subsubsection.5.2.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {11}{\ignorespaces Examples from the newly generated test set.}}{40}{table.11}}
\acronymused{SNLI}
\citation{kruszewski2015so}
\acronymused{SNLI}
\acronymused{POS}
\acronymused{SNLI}
\acronymused{SNLI}
\acronymused{SNLI}
\citation{maccartney2007natural}
\citation{maccartney2007natural}
\citation{bowman2015large}
\citation{bowman2015large}
\newlabel{tab:monoton_samples}{{5.2.1}{42}{}{table.11}{}}
\@writefile{lot}{\contentsline {table}{\numberline {12}{\ignorespaces Comparison of co-hyponyms in upward-monotone and downward-monone sentences.}}{42}{table.12}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.2.2}Validation}{42}{subsubsection.5.2.2}}
\AC@undonewlabel{acro:HIT}
\newlabel{acro:HIT}{{5.2.2}{42}{Validation}{section*.23}{}}
\acronymused{HIT}
\acronymused{HIT}
\acronymused{HIT}
\citation{landis1977measurement}
\citation{bowman2015large}
\citation{gong2017natural}
\acronymused{HIT}
\acronymused{SNLI}
\acronymused{HIT}
\acronymused{SNLI}
\acronymused{SNLI}
\acronymused{HIT}
\@writefile{lof}{\contentsline {figure}{\numberline {23}{\ignorespaces Example of a \ac {HIT} in Amazon Mechanical Turk.}}{43}{figure.23}}
\acronymused{HIT}
\newlabel{fig:example_hit}{{23}{43}{Example of a \ac {HIT} in Amazon Mechanical Turk}{figure.23}{}}
\acronymused{HIT}
\acronymused{SNLI}
\citation{gong2017natural}
\citation{nie2017shortcut}
\citation{chen2017enhanced}
\citation{parikh2016decomposable}
\citation{parikh2016decomposable}
\citation{nie2017shortcut}
\citation{chen2017enhanced}
\@writefile{lot}{\contentsline {table}{\numberline {13}{\ignorespaces Statistics of \ac {SNLI} testset compared with the newly generated testset.}}{44}{table.13}}
\acronymused{SNLI}
\newlabel{tab:newtest_stats}{{13}{44}{Statistics of \ac {SNLI} testset compared with the newly generated testset}{table.13}{}}
\acronymused{SNLI}
\acronymused{SNLI}
\acronymused{SNLI}
\acronymused{HIT}
\acronymused{SNLI}
\acronymused{SNLI}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Evaluation}{44}{subsection.5.3}}
\acronymused{SNLI}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.3.1}Experimental setup}{44}{subsubsection.5.3.1}}
\acronymused{biLSTM}
\acronymused{biLSTM}
\acronymused{SNLI}
\citation{chen2017natural}
\citation{parikh2016decomposable}
\citation{chen2017enhanced}
\citation{nie2017shortcut}
\citation{chen2017natural}
\newlabel{tab:compare_architecture_models}{{5.3.1}{45}{}{subsubsection.5.3.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {14}{\ignorespaces Architectural comparison of tested neural models without external knowledge.}}{45}{table.14}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.3.2}Models with external knowledge}{45}{subsubsection.5.3.2}}
\acronymused{KIM}
\acronymused{SNLI}
\acronymused{MultiNLI}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.3.3}Results}{45}{subsubsection.5.3.3}}
\acronymused{MultiNLI}
\acronymused{SNLI}
\acronymused{NLU}
\acronymused{MultiNLI}
\acronymused{KIM}
\citation{gururangan2018annotation}
\newlabel{tab:adv_results}{{5.3.3}{46}{Results}{subsubsection.5.3.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {15}{\ignorespaces Results of models on the new test set compared with the original \ac {SNLI} test set.}}{46}{table.15}}
\acronymused{SNLI}
\acronymused{KIM}
\acronymused{SNLI}
\acronymused{KIM}
\acronymused{KIM}
\acronymused{KIM}
\acronymused{NLU}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4}Analysis}{46}{subsection.5.4}}
\acronymused{MultiNLI}
\acronymused{SNLI}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.4.1}Accuracy by category}{46}{subsubsection.5.4.1}}
\acronymused{KIM}
\acronymused{SNLI}
\acronymused{SNLI}
\acronymused{SNLI}
\newlabel{tab:acc_by_cat}{{5.4.1}{47}{Accuracy by category}{subsubsection.5.4.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {16}{\ignorespaces Accuracy reached for the tested models for each category with assoziated sample words and the amount of instances.}}{47}{table.16}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.4.2}Impact on the word embeddings}{47}{subsubsection.5.4.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {24}{\ignorespaces TODO.}}{47}{figure.24}}
\newlabel{fig:decomp_acc_per_cos_sim}{{24}{47}{TODO}{figure.24}{}}
\citation{rubinstein2015well}
\citation{levy2015improving}
\@writefile{toc}{\contentsline {section}{\numberline {6}Approaches to incorporate WordNet information}{48}{section.6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Extraction of WordNet data}{48}{subsection.6.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Integrating information into word-embeddings}{48}{subsection.6.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.2.1}Motivation}{48}{subsubsection.6.2.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.2.2}Concatenating pre-trained word-embeddings}{48}{subsubsection.6.2.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.2.3}Concatenation categorical information}{48}{subsubsection.6.2.3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.2.4}Analysis}{48}{subsubsection.6.2.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3}Multitask Learning}{48}{subsection.6.3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.3.1}Motivation}{48}{subsubsection.6.3.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.3.2}Architecture}{48}{subsubsection.6.3.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.3.3}Approaches}{48}{subsubsection.6.3.3}}
\@writefile{toc}{\contentsline {paragraph}{\numberline {6.3.3.1}Different sizes of multitask MLP}{48}{paragraph.6.3.3.1}}
\@writefile{toc}{\contentsline {paragraph}{\numberline {6.3.3.2}Introducing Dropout}{48}{paragraph.6.3.3.2}}
\@writefile{toc}{\contentsline {paragraph}{\numberline {6.3.3.3}Introducing an additional shared layer}{48}{paragraph.6.3.3.3}}
\@writefile{toc}{\contentsline {paragraph}{\numberline {6.3.3.4}Fixing multitasking network during training}{48}{paragraph.6.3.3.4}}
\@writefile{toc}{\contentsline {paragraph}{\numberline {6.3.3.5}Focusing on original words within sentence representation}{48}{paragraph.6.3.3.5}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.3.4}Analysis}{48}{subsubsection.6.3.4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.3.5}Evaluation}{48}{subsubsection.6.3.5}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Conclusion}{49}{section.7}}
\bibdata{bib/literature}
\bibcite{balazs2017refining}{{1}{2017}{{Balazs et~al.}}{{Balazs, Marrese-Taylor, Loyola, and Matsuo}}}
\bibcite{bengio2013representation}{{2}{2013}{{Bengio et~al.}}{{Bengio, Courville, and Vincent}}}
\bibcite{Bishop2007}{{3}{2007}{{Bishop}}{{}}}
\bibcite{bos2005recognising}{{4}{2005}{{Bos and Markert}}{{}}}
\bibcite{bowman2015large}{{5}{2015}{{Bowman et~al.}}{{Bowman, Angeli, Potts, and Manning}}}
\bibcite{bowman2016fast}{{6}{2016}{{Bowman et~al.}}{{Bowman, Gauthier, Rastogi, Gupta, Manning, and Potts}}}
\bibcite{bromley1994signature}{{7}{1994}{{Bromley et~al.}}{{Bromley, Guyon, LeCun, S{\"a}ckinger, and Shah}}}
\bibcite{celikyilmaz2010enriching}{{8}{2010}{{Celikyilmaz et~al.}}{{Celikyilmaz, Hakkani-Tur, Pasupat, and Sarikaya}}}
\bibcite{chatzikyriakidis2017overview}{{9}{2017}{{Chatzikyriakidis et~al.}}{{Chatzikyriakidis, Cooper, Dobnik, and Larsson}}}
\bibcite{chen2017natural}{{10}{2017{a}}{{Chen et~al.}}{{Chen, Zhu, Ling, and Inkpen}}}
\bibcite{chen2017enhanced}{{11}{2017{b}}{{Chen et~al.}}{{Chen, Zhu, Ling, Wei, Jiang, and Inkpen}}}
\bibcite{chen2017recurrent}{{12}{2017{c}}{{Chen et~al.}}{{Chen, Zhu, Ling, Wei, Jiang, and Inkpen}}}
\bibcite{cheng2016long}{{13}{2016}{{Cheng et~al.}}{{Cheng, Dong, and Lapata}}}
\bibcite{clark2016combining}{{14}{2016}{{Clark et~al.}}{{Clark, Etzioni, Khot, Sabharwal, Tafjord, Turney, and Khashabi}}}
\bibcite{cooper1996using}{{15}{1996}{{Cooper et~al.}}{{Cooper, Crouch, Van~Eijck, Fox, Van~Genabith, Jaspars, Kamp, Milward, Pinkal, Poesio et~al.}}}
\bibcite{dagan2000contextual}{{16}{2000}{{Dagan}}{{}}}
\bibcite{dagan2009recognizing}{{17}{2009}{{Dagan et~al.}}{{Dagan, Dolan, Magnini, and Roth}}}
\bibcite{dagan2006pascal}{{18}{2006}{{Dagan et~al.}}{{Dagan, Glickman, and Magnini}}}
\bibcite{dasgupta2018evaluating}{{19}{2018}{{Dasgupta et~al.}}{{Dasgupta, Guo, Stuhlm{\"u}ller, Gershman, and Goodman}}}
\bibcite{faruqui2015retrofitting}{{20}{2015}{{Faruqui et~al.}}{{Faruqui, Dodge, Jauhar, Dyer, Hovy, and Smith}}}
\bibcite{ghaeini2018dr}{{21}{2018}{{Ghaeini et~al.}}{{Ghaeini, Hasan, Datla, Liu, Lee, Qadir, Ling, Prakash, Fern, and Farri}}}
\bibcite{glockner_acl18}{{22}{2018}{{Glockner et~al.}}{{Glockner, Shwartz, and Goldberg}}}
\bibcite{goldberg2017Apr}{{23}{2017}{{Goldberg}}{{}}}
\bibcite{gong2017natural}{{24}{2017}{{Gong et~al.}}{{Gong, Luo, and Zhang}}}
\bibcite{graves2005framewise}{{25}{2005}{{Graves and Schmidhuber}}{{}}}
\bibcite{gurevych2012uby}{{26}{2012}{{Gurevych et~al.}}{{Gurevych, Eckle-Kohler, Hartmann, Matuschek, Meyer, and Wirth}}}
\bibcite{gurevych2016linked}{{27}{2016}{{Gurevych et~al.}}{{Gurevych, Eckle-Kohler, and Matuschek}}}
\bibcite{gururangan2018annotation}{{28}{2018}{{Gururangan et~al.}}{{Gururangan, Swayamdipta, Levy, Schwartz, Bowman, and Smith}}}
\bibcite{hochreiter1997long}{{29}{1997}{{Hochreiter and Schmidhuber}}{{}}}
\bibcite{hu2016deep}{{30}{2016}{{Hu et~al.}}{{Hu, Yang, Salakhutdinov, and Xing}}}
\bibcite{ide2001american}{{31}{2001}{{Ide and Macleod}}{{}}}
\bibcite{ide2004american}{{32}{2004}{{Ide and Suderman}}{{}}}
\bibcite{ide2006integrating}{{33}{2006}{{Ide and Suderman}}{{}}}
\bibcite{im2017distance}{{34}{2017}{{Im and Cho}}{{}}}
\bibcite{jia-liang:2017:EMNLP2017}{{35}{2017}{{Jia and Liang}}{{}}}
\bibcite{Jurafsky2008May}{{36}{2008}{{Jurafsky and Martin}}{{}}}
\bibcite{scitail}{{37}{2018}{{Khot et~al.}}{{Khot, Sabharwal, and Clark}}}
\bibcite{kingma2014adam}{{38}{2014}{{Kingma and Ba}}{{}}}
\bibcite{kruszewski2015so}{{39}{2015}{{Kruszewski and Baroni}}{{}}}
\bibcite{lecun2015deep}{{40}{2015}{{LeCun et~al.}}{{LeCun, Bengio, and Hinton}}}
\bibcite{levy2015improving}{{41}{2015}{{Levy et~al.}}{{Levy, Goldberg, and Dagan}}}
\bibcite{liu2015learning}{{42}{2015}{{Liu et~al.}}{{Liu, Jiang, Wei, Ling, and Hu}}}
\bibcite{maccartney2008phrase}{{43}{2008}{{MacCartney et~al.}}{{MacCartney, Galley, and Manning}}}
\bibcite{maccartney2007natural}{{44}{2007}{{MacCartney and Manning}}{{}}}
\bibcite{marcus1993building}{{45}{1993}{{Marcus et~al.}}{{Marcus, Marcinkiewicz, and Santorini}}}
\bibcite{marelli2014semeval}{{46}{2014}{{Marelli et~al.}}{{Marelli, Bentivogli, Baroni, Bernardi, Menini, and Zamparelli}}}
\bibcite{mccarthy2004using}{{47}{2004}{{McCarthy et~al.}}{{McCarthy, Koeling, Weeds, and Carroll}}}
\bibcite{mikolov2013distributed}{{48}{2013{a}}{{Mikolov et~al.}}{{Mikolov, Sutskever, Chen, Corrado, and Dean}}}
\bibcite{mikolov2013linguistic}{{49}{2013{b}}{{Mikolov et~al.}}{{Mikolov, Yih, and Zweig}}}
\bibcite{miller1995wordnet}{{50}{1995}{{Miller}}{{}}}
\bibcite{mou2015natural}{{51}{2015}{{Mou et~al.}}{{Mou, Men, Li, Xu, Zhang, Yan, and Jin}}}
\bibcite{mrkvsic2017semantic}{{52}{2017}{{Mrk{\v {s}}i{\'c} et~al.}}{{Mrk{\v {s}}i{\'c}, Vuli{\'c}, S{\'e}aghdha, Leviant, Reichart, Ga{\v {s}}i{\'c}, Korhonen, and Young}}}
\bibcite{munkhdalai2017neural}{{53}{2017}{{Munkhdalai and Yu}}{{}}}
\bibcite{murphy2003semantic}{{54}{2003}{{Murphy}}{{}}}
\bibcite{nangia2017repeval}{{55}{2017}{{Nangia et~al.}}{{Nangia, Williams, Lazaridou, and Bowman}}}
\bibcite{nie2017shortcut}{{56}{2017}{{Nie and Bansal}}{{}}}
\bibcite{pantel2005inducing}{{57}{2005}{{Pantel}}{{}}}
\bibcite{parikh2016decomposable}{{58}{2016}{{Parikh et~al.}}{{Parikh, T{\"a}ckstr{\"o}m, Das, and Uszkoreit}}}
\bibcite{pennington2014glove}{{59}{2014}{{Pennington et~al.}}{{Pennington, Socher, and Manning}}}
\bibcite{peters2018deep}{{60}{2018}{{Peters et~al.}}{{Peters, Neumann, Iyyer, Gardner, Clark, Lee, and Zettlemoyer}}}
\bibcite{prakash2007learning}{{61}{2007}{{Prakash et~al.}}{{Prakash, Jurafsky, and Ng}}}
\bibcite{resnik1995using}{{62}{1995}{{Resnik}}{{}}}
\bibcite{rocktaschel2015reasoning}{{63}{2015}{{Rockt{\"a}schel et~al.}}{{Rockt{\"a}schel, Grefenstette, Hermann, Ko{\v {c}}isk{\`y}, and Blunsom}}}
\bibcite{rubinstein2015well}{{64}{2015}{{Rubinstein et~al.}}{{Rubinstein, Levi, Schwartz, and Rappoport}}}
\bibcite{ruckle2018concatenated}{{65}{2018}{{R{\"u}ckl{\'e} et~al.}}{{R{\"u}ckl{\'e}, Eger, Peyrard, and Gurevych}}}
\bibcite{sahlgren2008distributional}{{66}{2008}{{Sahlgren}}{{}}}
\bibcite{shen2018reinforced}{{67}{2018}{{Shen et~al.}}{{Shen, Zhou, Long, Jiang, Wang, and Zhang}}}
\bibcite{shwartz2015learning}{{68}{2015}{{Shwartz et~al.}}{{Shwartz, Levy, Dagan, and Goldberger}}}
\bibcite{suchanek2007yago}{{69}{2007}{{Suchanek et~al.}}{{Suchanek, Kasneci, and Weikum}}}
\bibcite{tatu2005semantic}{{70}{2005}{{Tatu and Moldovan}}{{}}}
\bibcite{tay2017compare}{{71}{2017}{{Tay et~al.}}{{Tay, Tuan, and Hui}}}
\bibcite{vulic2017specialising}{{72}{2017}{{Vuli{\'c} and Mrk{\v {s}}i{\'c}}}{{}}}
\bibcite{vulic2017morph}{{73}{2017}{{Vuli{\'c} et~al.}}{{Vuli{\'c}, Mrk{\v {s}}i{\'c}, Reichart, S{\'e}aghdha, Young, and Korhonen}}}
\bibcite{welbl2017crowdsourcing}{{74}{2017}{{Welbl et~al.}}{{Welbl, Liu, and Gardner}}}
\bibcite{williams2017broad}{{75}{2017}{{Williams et~al.}}{{Williams, Nangia, and Bowman}}}
\bibcite{xu2014rc}{{76}{2014}{{Xu et~al.}}{{Xu, Bai, Bian, Gao, Wang, Liu, and Liu}}}
\bibcite{yang2017character}{{77}{2017}{{Yang et~al.}}{{Yang, Costa-juss{\`a}, and Fonollosa}}}
\bibcite{young2014image}{{78}{2014}{{Young et~al.}}{{Young, Lai, Hodosh, and Hockenmaier}}}
\bibcite{zesch2008extracting}{{79}{2008}{{Zesch et~al.}}{{Zesch, M{\"u}ller, and Gurevych}}}
\citation{nie2017shortcut}
\acronymused{RNN}
\acronymused{RNN}
\acronymused{POS}
\acronymused{HIT}
\citation{nie2017shortcut}
\acronymused{SNLI}
\acronymused{MultiNLI}
\acronymused{SNLI}
\acronymused{SNLI}
\acronymused{SNLI}
\acronymused{SNLI}
\global\@namedef{scr@dte@section@lastmaxnumwidth}{9.35994pt}
\global\@namedef{scr@dte@subsection@lastmaxnumwidth}{17.00493pt}
\global\@namedef{scr@dte@subsubsection@lastmaxnumwidth}{24.92792pt}
\global\@namedef{scr@dte@table@lastmaxnumwidth}{14.36394pt}
\global\@namedef{scr@dte@figure@lastmaxnumwidth}{14.36394pt}
