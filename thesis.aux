\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand*\new@tpo@label[2]{}
\bibstyle{acl_natbib}
\AC@reset@newl@bel
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\babel@aux{english}{}
\newacro{biLSTM}[\AC@hyperlink{biLSTM}{biLSTM}]{bidirectional Long-Short-Term Memory Network}
\newacro{BoW}[\AC@hyperlink{BoW}{BoW}]{Bag of Words}
\newacro{ESIM}[\AC@hyperlink{ESIM}{ESIM}]{Enhanced Sequential Inference Model}
\newacro{HIT}[\AC@hyperlink{HIT}{HIT}]{Human Intelligence Task}
\newacro{IE}[\AC@hyperlink{IE}{IE}]{Information Extraction}
\newacro{IR}[\AC@hyperlink{IR}{IR}]{Information Retrieval}
\newacro{KIM}[\AC@hyperlink{KIM}{KIM}]{Knowledge-based Inference Model}
\newacro{LSTM}[\AC@hyperlink{LSTM}{LSTM}]{Long-Short-Term-Memory}
\newacro{MLP}[\AC@hyperlink{MLP}{MLP}]{Multi Layer Perceptron}
\newacro{MSE}[\AC@hyperlink{MSE}{MSE}]{Mean Squared Error}
\newacro{MultiNLI}[\AC@hyperlink{MultiNLI}{MultiNLI}]{MultiGenre Natural Language Inference Corpus}
\newacro{NLI}[\AC@hyperlink{NLI}{NLI}]{Natural Language Inference}
\newacro{NLP}[\AC@hyperlink{NLP}{NLP}]{Natural Language Processing}
\newacro{NLU}[\AC@hyperlink{NLU}{NLU}]{Natural Language Understanding}
\newacro{POS}[\AC@hyperlink{POS}{POS}]{Part of Speech}
\newacro{OANC}[\AC@hyperlink{OANC}{OANC}]{Open American National Corpus}
\newacro{QA}[\AC@hyperlink{QA}{QA}]{Question Answering}
\newacro{ReLU}[\AC@hyperlink{ReLU}{ReLU}]{Rectified Linear Units}
\newacro{RNN}[\AC@hyperlink{RNN}{RNN}]{Recurrent Neural Network}
\newacro{RTE}[\AC@hyperlink{RTE}{RTE}]{Recognizing Textual Entailment}
\newacro{SD}[\AC@hyperlink{SD}{SD}]{Standard Deviation}
\newacro{SICK}[\AC@hyperlink{SICK}{SICK}]{Sentences Involving Compositional Knowledge}
\newacro{SNLI}[\AC@hyperlink{SNLI}{SNLI}]{The Stanford Natural Language Inference Corpus}
\newacro{WSD}[\AC@hyperlink{WSD}{WSD}]{Word Sense Disambiguation}
\newacro{YAGO}[\AC@hyperlink{YAGO}{YAGO}]{Yet Another Great Ontology}
\AC@undonewlabel{acro:NLI}
\newlabel{acro:NLI}{{}{5}{}{section*.1}{}}
\acronymused{NLI}
\citation{bengio2013representation}
\citation{mikolov2013distributed}
\citation{pennington2014glove}
\citation{celikyilmaz2010enriching}
\citation{vulic2017morph}
\citation{bowman2015large}
\citation{dagan2006pascal}
\citation{maccartney2007natural}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{7}{section.1}}
\AC@undonewlabel{acro:NLP}
\newlabel{acro:NLP}{{1}{7}{Introduction}{section*.2}{}}
\acronymused{NLP}
\AC@undonewlabel{acro:NLU}
\newlabel{acro:NLU}{{1}{7}{Introduction}{section*.3}{}}
\acronymused{NLU}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Goal of this thesis}{7}{subsection.1.1}}
\acronymused{NLI}
\AC@undonewlabel{acro:RTE}
\newlabel{acro:RTE}{{1.1}{7}{Goal of this thesis}{section*.4}{}}
\acronymused{RTE}
\acronymused{NLU}
\acronymused{NLP}
\acronymused{NLI}
\acronymused{NLI}
\acronymused{NLU}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Structure of this thesis}{7}{subsection.1.2}}
\AC@undonewlabel{acro:LSTM}
\newlabel{acro:LSTM}{{1.2}{7}{Structure of this thesis}{section*.5}{}}
\acronymused{LSTM}
\AC@undonewlabel{acro:RNN}
\newlabel{acro:RNN}{{1.2}{7}{Structure of this thesis}{section*.6}{}}
\acronymused{RNN}
\acronymused{NLP}
\acronymused{NLI}
\acronymused{NLI}
\acronymused{NLU}
\acronymused{NLI}
\citation{bowman2015large}
\citation{dagan2009recognizing}
\citation{maccartney2008phrase}
\citation{bowman2015large}
\citation{maccartney2007natural,bos2005recognising}
\citation{dagan2009recognizing}
\citation{williams2017broad,cooper1996using,bos2005recognising,dagan2006pascal}
\@writefile{toc}{\contentsline {section}{\numberline {2}Theoretical Background}{9}{section.2}}
\newlabel{sec:basics}{{2}{9}{Theoretical Background}{section.2}{}}
\acronymused{NLI}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Natural Language Inference}{9}{subsection.2.1}}
\newlabel{sec:basics_nli}{{2.1}{9}{Natural Language Inference}{subsection.2.1}{}}
\acronymused{NLI}
\acronymused{NLI}
\acronymused{NLI}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Example sentence-pairs for each possible label, taken from \href  {https://nlp.stanford.edu/projects/snli/}{SNLI Leaderboard}}}{9}{table.1}}
\newlabel{tab:label_examples}{{1}{9}{Example sentence-pairs for each possible label, taken from \href {https://nlp.stanford.edu/projects/snli/}{SNLI Leaderboard}}{table.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.1}Relatedness to other NLP tasks}{9}{subsubsection.2.1.1}}
\acronymused{NLI}
\acronymused{NLP}
\citation{murphy2003semantic}
\citation{dagan2009recognizing}
\citation{Jurafsky2008May}
\citation{Jurafsky2008May}
\citation{Jurafsky2008May}
\acronymused{NLU}
\acronymused{NLP}
\AC@undonewlabel{acro:QA}
\newlabel{acro:QA}{{2.1.1}{10}{Relatedness to other NLP tasks}{section*.7}{}}
\acronymused{QA}
\AC@undonewlabel{acro:IE}
\newlabel{acro:IE}{{2.1.1}{10}{Relatedness to other NLP tasks}{section*.8}{}}
\acronymused{IE}
\acronymused{QA}
\acronymused{IE}
\acronymused{NLP}
\acronymused{NLU}
\acronymused{NLU}
\acronymused{NLI}
\acronymused{NLU}
\acronymused{NLU}
\acronymused{NLP}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Lexical Semantic Relations}{10}{subsection.2.2}}
\newlabel{sec:word_relations}{{2.2}{10}{Lexical Semantic Relations}{subsection.2.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.1}Synonymy and antonomy}{10}{subsubsection.2.2.1}}
\citation{Jurafsky2008May}
\citation{shwartz2015learning}
\citation{shwartz2015learning}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces A sample ontology of animals to illustrate the lexical relations \textit  {Hypernomy} and \textit  {Holonymy}.}}{11}{figure.1}}
\newlabel{fig:lexical_resources}{{1}{11}{A sample ontology of animals to illustrate the lexical relations \textit {Hypernomy} and \textit {Holonymy}}{figure.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.2}Hypernomy}{11}{subsubsection.2.2.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.3}Holonomy}{11}{subsubsection.2.2.3}}
\acronymused{NLI}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.4}Lexical semantic realtions for \ac {NLI}}{11}{subsubsection.2.2.4}}
\acronymused{NLI}
\acronymused{NLI}
\citation{nie2017shortcut}
\citation{bromley1994signature}
\citation{nie2017shortcut}
\citation{nie2017shortcut}
\citation{hochreiter1997long}
\citation{graves2005framewise}
\citation{nie2017shortcut}
\citation{nie2017shortcut}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Shortcut-Stacked-Encoder and Residual Encoder}{12}{subsection.2.3}}
\newlabel{sec:residual_encoder_def}{{2.3}{12}{Shortcut-Stacked-Encoder and Residual Encoder}{subsection.2.3}{}}
\acronymused{NLI}
\AC@undonewlabel{acro:MLP}
\newlabel{acro:MLP}{{2.3}{12}{Shortcut-Stacked-Encoder and Residual Encoder}{section*.9}{}}
\acronymused{MLP}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.1}Sentence Encoding for Shortcut-Stacked-Encoder}{12}{subsubsection.2.3.1}}
\acronymused{NLI}
\AC@undonewlabel{acro:biLSTM}
\newlabel{acro:biLSTM}{{2.3.1}{12}{Sentence Encoding for Shortcut-Stacked-Encoder}{section*.10}{}}
\acronymused{biLSTM}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces The architecture of the sentence-encoding component within the Shortcut-Stacked-Encoder, taken from \cite  {nie2017shortcut}.}}{12}{figure.2}}
\newlabel{fig:sentence_emcoder_shortcut}{{2}{12}{The architecture of the sentence-encoding component within the Shortcut-Stacked-Encoder, taken from \cite {nie2017shortcut}}{figure.2}{}}
\acronymused{LSTM}
\acronymused{biLSTM}
\citation{mou2015natural}
\citation{nie2017shortcut}
\citation{kingma2014adam}
\citation{pennington2014glove}
\acronymused{biLSTM}
\acronymused{biLSTM}
\acronymused{biLSTM}
\acronymused{biLSTM}
\acronymused{biLSTM}
\newlabel{eq:stacked_encoder_input}{{2}{13}{Sentence Encoding for Shortcut-Stacked-Encoder}{equation.2.2}{}}
\acronymused{biLSTM}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.2}Classification}{13}{subsubsection.2.3.2}}
\acronymused{MLP}
\AC@undonewlabel{acro:ReLU}
\newlabel{acro:ReLU}{{2.3.2}{13}{Classification}{section*.11}{}}
\acronymused{ReLU}
\acronymused{MLP}
\acronymused{NLI}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.3}Training}{13}{subsubsection.2.3.3}}
\acronymused{MLP}
\acronymused{biLSTM}
\citation{nie2017shortcut}
\citation{gong2017natural}
\citation{gong2017natural}
\citation{nie2017shortcut}
\citation{nie2017shortcut}
\citation{nie2017shortcut}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.4}Residual Encoder and Reimplementation Variants}{14}{subsubsection.2.3.4}}
\acronymused{biLSTM}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Accuracy in percent of different implementations of the model from \cite  {nie2017shortcut}, achieved on the SNLI dataset compared with human performance.}}{14}{table.2}}
\newlabel{table:reimplementation_performance}{{2}{14}{Accuracy in percent of different implementations of the model from \cite {nie2017shortcut}, achieved on the SNLI dataset compared with human performance}{table.2}{}}
\acronymused{biLSTM}
\acronymused{MLP}
\acronymused{biLSTM}
\acronymused{MLP}
\acronymused{biLSTM}
\acronymused{MLP}
\acronymused{NLI}
\citation{bos2005recognising,tatu2005semantic}
\citation{miller1995wordnet}
\citation{Jurafsky2008May}
\citation{Jurafsky2008May}
\citation{mccarthy2004using}
\citation{resnik1995using}
\citation{prakash2007learning}
\@writefile{toc}{\contentsline {section}{\numberline {3}Related Work}{16}{section.3}}
\newlabel{sec:related_work}{{3}{16}{Related Work}{section.3}{}}
\acronymused{NLI}
\acronymused{NLI}
\acronymused{NLP}
\acronymused{NLI}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}External Resources}{16}{subsection.3.1}}
\newlabel{sec:ext_resources}{{3.1}{16}{External Resources}{subsection.3.1}{}}
\acronymused{NLI}
\acronymused{NLP}
\acronymused{NLI}
\acronymused{NLU}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.1}WordNet}{16}{subsubsection.3.1.1}}
\newlabel{sec:wordnet}{{3.1.1}{16}{WordNet}{subsubsection.3.1.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Example of different synsets of the lemma ``table'' (only noun senses) within WordNet, taken from \href  {http://wordnetweb.princeton.edu}{http://wordnetweb.princeton.edu}.}}{16}{figure.3}}
\newlabel{fig:wordnet}{{3}{16}{Example of different synsets of the lemma ``table'' (only noun senses) within WordNet, taken from \href {http://wordnetweb.princeton.edu}{http://wordnetweb.princeton.edu}}{figure.3}{}}
\AC@undonewlabel{acro:POS}
\newlabel{acro:POS}{{3.1.1}{16}{WordNet}{section*.12}{}}
\acronymused{POS}
\citation{gurevych2016linked}
\citation{zesch2008extracting}
\citation{gurevych2016linked}
\citation{suchanek2007yago}
\citation{gurevych2012uby}
\AC@undonewlabel{acro:WSD}
\newlabel{acro:WSD}{{3.1.1}{17}{}{section*.13}{}}
\acronymused{WSD}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.2}Wikipedia}{17}{subsubsection.3.1.2}}
\newlabel{sec:wikipedia}{{3.1.2}{17}{Wikipedia}{subsubsection.3.1.2}{}}
\acronymused{NLP}
\acronymused{NLI}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.3}Derived from multiple Knowledge Bases}{17}{subsubsection.3.1.3}}
\AC@undonewlabel{acro:YAGO}
\newlabel{acro:YAGO}{{3.1.3}{17}{Derived from multiple Knowledge Bases}{section*.14}{}}
\acronymused{YAGO}
\acronymused{YAGO}
\acronymused{YAGO}
\citation{cooper1996using}
\citation{dagan2006pascal}
\citation{marelli2014semeval}
\citation{young2014image}
\citation{bowman2015large}
\citation{bowman2015large}
\citation{bowman2015large}
\citation{young2014image}
\acronymused{NLP}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Datasets for NLI}{18}{subsection.3.2}}
\newlabel{sec:basics_datasets}{{3.2}{18}{Datasets for NLI}{subsection.3.2}{}}
\acronymused{NLI}
\AC@undonewlabel{acro:SNLI}
\newlabel{acro:SNLI}{{3.2}{18}{Datasets for NLI}{section*.15}{}}
\acronymused{SNLI}
\acronymused{NLI}
\AC@undonewlabel{acro:SICK}
\newlabel{acro:SICK}{{3.2}{18}{Datasets for NLI}{section*.16}{}}
\acronymused{SICK}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1}SNLI}{18}{subsubsection.3.2.1}}
\newlabel{sec:snli}{{3.2.1}{18}{SNLI}{subsubsection.3.2.1}{}}
\acronymused{SNLI}
\acronymused{NLI}
\acronymused{NLI}
\acronymused{SNLI}
\acronymused{SNLI}
\citation{gururangan2018annotation}
\acronymused{SNLI}
\acronymused{SNLI}
\acronymused{SNLI}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Example sentence pairs, taken from \ac {SNLI}, showing typical sentences within the dataset.}}{19}{table.3}}
\acronymused{SNLI}
\newlabel{table:snli_example}{{3}{19}{Example sentence pairs, taken from \ac {SNLI}, showing typical sentences within the dataset}{table.3}{}}
\AC@undonewlabel{acro:BoW}
\newlabel{acro:BoW}{{3.2.1}{19}{}{section*.17}{}}
\acronymused{BoW}
\acronymused{SNLI}
\citation{chatzikyriakidis2017overview,williams2017broad}
\citation{williams2017broad}
\citation{bowman2015large}
\citation{ide2001american,ide2004american,ide2006integrating}
\citation{nangia2017repeval}
\citation{williams2017broad}
\citation{chen2017recurrent}
\citation{nie2017shortcut}
\citation{nie2017shortcut,balazs2017refining,yang2017character}
\citation{scitail}
\acronymused{SNLI}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.2}MultiNLI}{20}{subsubsection.3.2.2}}
\acronymused{SNLI}
\acronymused{SNLI}
\AC@undonewlabel{acro:MultiNLI}
\newlabel{acro:MultiNLI}{{3.2.2}{20}{MultiNLI}{section*.18}{}}
\acronymused{MultiNLI}
\acronymused{MultiNLI}
\AC@undonewlabel{acro:OANC}
\newlabel{acro:OANC}{{3.2.2}{20}{}{section*.19}{}}
\acronymused{OANC}
\acronymused{MultiNLI}
\acronymused{NLI}
\acronymused{NLU}
\acronymused{SNLI}
\acronymused{NLU}
\acronymused{MultiNLI}
\acronymused{SNLI}
\acronymused{SNLI}
\acronymused{MultiNLI}
\acronymused{SNLI}
\acronymused{MultiNLI}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.3}SciTail}{20}{subsubsection.3.2.3}}
\acronymused{NLI}
\acronymused{SNLI}
\acronymused{OANC}
\citation{scitail}
\citation{welbl2017crowdsourcing}
\citation{scitail}
\citation{clark2016combining}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Example sentence pairs from \ac {MultiNLI}, taken from RepEval 2017 Shared Task, showing samples of different genres.}}{21}{table.4}}
\acronymused{MultiNLI}
\newlabel{table:multinli_example}{{4}{21}{Example sentence pairs from \ac {MultiNLI}, taken from RepEval 2017 Shared Task, showing samples of different genres}{table.4}{}}
\acronymused{QA}
\acronymused{SNLI}
\acronymused{MultiNLI}
\acronymused{NLI}
\acronymused{QA}
\AC@undonewlabel{acro:IR}
\newlabel{acro:IR}{{2}{21}{}{section*.20}{}}
\acronymused{IR}
\acronymused{QA}
\acronymused{NLI}
\acronymused{QA}
\acronymused{NLU}
\citation{bromley1994signature}
\citation{bowman2016fast}
\citation{bowman2016fast}
\citation{munkhdalai2017neural}
\citation{chen2017recurrent}
\citation{nie2017shortcut}
\citation{shen2018reinforced,im2017distance}
\citation{shen2018reinforced}
\citation{shen2018reinforced}
\citation{shen2018reinforced}
\citation{im2017distance}
\citation{rocktaschel2015reasoning}
\citation{cheng2016long}
\citation{parikh2016decomposable}
\citation{chen2017enhanced}
\citation{parikh2016decomposable}
\citation{mou2015natural}
\citation{chen2017enhanced}
\citation{chen2017enhanced}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Neural Models for NLI}{22}{subsection.3.3}}
\newlabel{sec:models_snli}{{3.3}{22}{Neural Models for NLI}{subsection.3.3}{}}
\acronymused{SNLI}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.1}Sentence Encoding Models}{22}{subsubsection.3.3.1}}
\acronymused{MLP}
\acronymused{biLSTM}
\acronymused{SNLI}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.2}Inter-sentence-attention-based models}{22}{subsubsection.3.3.2}}
\newlabel{sec:rel_work_sentence_encoding_models}{{3.3.2}{22}{Inter-sentence-attention-based models}{subsubsection.3.3.2}{}}
\acronymused{NLI}
\acronymused{SNLI}
\citation{chen2017natural}
\citation{chen2017natural}
\citation{chen2017natural}
\citation{chen-EtAl:2017b:natural}
\citation{chen2017natural}
\citation{tay2017compare,peters2018deep,ghaeini2018dr}
\citation{im2017distance}
\citation{gong2017natural}
\citation{hu2016deep}
\acronymused{BoW}
\acronymused{LSTM}
\AC@undonewlabel{acro:ESIM}
\newlabel{acro:ESIM}{{3.3.2}{23}{}{section*.21}{}}
\acronymused{ESIM}
\acronymused{biLSTM}
\acronymused{biLSTM}
\acronymused{MLP}
\acronymused{biLSTM}
\acronymused{SNLI}
\AC@undonewlabel{acro:KIM}
\newlabel{acro:KIM}{{3.3.2}{23}{}{section*.22}{}}
\acronymused{KIM}
\acronymused{SNLI}
\acronymused{SNLI}
\acronymused{SNLI}
\acronymused{SNLI}
\acronymused{KIM}
\citation{xu2014rc}
\citation{goldberg2014word2vec}
\citation{liu2015learning}
\citation{faruqui2015retrofitting}
\citation{mrkvsic2017semantic}
\citation{vulic2017specialising}
\citation{levy2015improving}
\citation{ruckle2018concatenated}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Integration of external Resources into Neural Networks}{24}{subsection.3.4}}
\newlabel{sec:ext_res_in_nn}{{3.4}{24}{Integration of external Resources into Neural Networks}{subsection.3.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.1}Improving word-embeddings}{24}{subsubsection.3.4.1}}
\newlabel{sec:embeddings_improvements_relwork}{{3.4.1}{24}{Improving word-embeddings}{subsubsection.3.4.1}{}}
\acronymused{NLI}
\acronymused{SNLI}
\citation{goldberg2017Apr}
\citation{shen2018reinforced}
\citation{im2017distance}
\citation{goldberg2017Apr}
\citation{goldberg2017Apr}
\citation{nie2017shortcut}
\@writefile{toc}{\contentsline {section}{\numberline {4}Understanding Shortcut-Stacked-Encoder}{26}{section.4}}
\newlabel{sec:understanding}{{4}{26}{Understanding Shortcut-Stacked-Encoder}{section.4}{}}
\acronymused{SNLI}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Motivation}{26}{subsection.4.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Insights on the sentence representation}{26}{subsection.4.2}}
\newlabel{sec:insights_sent_repr}{{4.2}{26}{Insights on the sentence representation}{subsection.4.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.1}Approach}{26}{subsubsection.4.2.1}}
\acronymused{SNLI}
\newlabel{sec:understanding1_method}{{4.2.1}{26}{}{subsubsection.4.2.1}{}}
\acronymused{LSTM}
\acronymused{RNN}
\acronymused{RNN}
\acronymused{LSTM}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces General architecture of a \ac {RNN} (left). Example sentence in an unrolled \ac {RNN} (right).}}{27}{figure.4}}
\acronymused{RNN}
\acronymused{RNN}
\newlabel{fig:rnn}{{4}{27}{General architecture of a \ac {RNN} (left). Example sentence in an unrolled \ac {RNN} (right)}{figure.4}{}}
\acronymused{RNN}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Visualized example of extracting interpretable information of the max-pooled seentence representations with a dimensionality of 3.}}{27}{figure.5}}
\newlabel{fig:example_process_understanding}{{5}{27}{Visualized example of extracting interpretable information of the max-pooled seentence representations with a dimensionality of 3}{figure.5}{}}
\acronymused{biLSTM}
\newlabel{sec:understanding1_analysed_data}{{4.2.1}{27}{}{figure.5}{}}
\acronymused{SNLI}
\citation{Bishop2007}
\acronymused{POS}
\acronymused{POS}
\acronymused{POS}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.2}Detection of relevant dimensions}{28}{subsubsection.4.2.2}}
\AC@undonewlabel{acro:SD}
\newlabel{acro:SD}{{4.2.2}{28}{Detection of relevant dimensions}{section*.23}{}}
\acronymused{SD}
\acronymused{SD}
\acronymused{SD}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces The standard deviation within a dimension of sentence representations (x-axis) by the amount of dimensions with the given standard deviation.}}{28}{figure.6}}
\newlabel{fig:sd}{{6}{28}{The standard deviation within a dimension of sentence representations (x-axis) by the amount of dimensions with the given standard deviation}{figure.6}{}}
\acronymused{SD}
\acronymused{SD}
\citation{dagan2000contextual}
\acronymused{NLI}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces An extraction of a grid-plot, showing dimensions with the position within the sentence of the word, responsible for the dimensional value.}}{29}{figure.7}}
\newlabel{fig:find_position_1}{{7}{29}{An extraction of a grid-plot, showing dimensions with the position within the sentence of the word, responsible for the dimensional value}{figure.7}{}}
\acronymused{SD}
\acronymused{SD}
\acronymused{SD}
\citation{marcus1993building}
\acronymused{SNLI}
\acronymused{SNLI}
\acronymused{POS}
\acronymused{POS}
\acronymused{POS}
\acronymused{SD}
\citation{gururangan2018annotation}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces An extraction of a grid-plot, showing syntatical information using the \ac {POS} tag with pre-sorted rows to have a single dominant label.}}{31}{figure.8}}
\acronymused{POS}
\newlabel{fig:find_syntax}{{8}{31}{An extraction of a grid-plot, showing syntatical information using the \ac {POS} tag with pre-sorted rows to have a single dominant label}{figure.8}{}}
\acronymused{POS}
\acronymused{SD}
\acronymused{SD}
\citation{mikolov2013linguistic}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces An extraction of a grid-plot, gender specific female using only sentences with words of pre-defined wordlists.}}{32}{figure.9}}
\newlabel{fig:find_male_female}{{9}{32}{An extraction of a grid-plot, gender specific female using only sentences with words of pre-defined wordlists}{figure.9}{}}
\acronymused{SD}
\acronymused{SNLI}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.3}Female and male dimensions}{32}{subsubsection.4.2.3}}
\newlabel{sec:understanding2}{{4.2.3}{32}{Female and male dimensions}{subsubsection.4.2.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Representation visualitation with respect to genders of dimension \textit  {199} (left) and dimension \textit  {602} (right).}}{33}{figure.10}}
\newlabel{fig:mf_basic}{{10}{33}{Representation visualitation with respect to genders of dimension \textit {199} (left) and dimension \textit {602} (right)}{figure.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Detailed representation visualitation of different terms for human males of dimension \textit  {199} (left) and dimension \textit  {602} (right).}}{33}{figure.11}}
\newlabel{fig:mf_detailed_m}{{11}{33}{Detailed representation visualitation of different terms for human males of dimension \textit {199} (left) and dimension \textit {602} (right)}{figure.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Detailed representation visualitation of different terms for human females of dimension \textit  {845} (left) and dimension \textit  {311} (right).}}{34}{figure.12}}
\newlabel{fig:mf_detailed_f}{{12}{34}{Detailed representation visualitation of different terms for human females of dimension \textit {845} (left) and dimension \textit {311} (right)}{figure.12}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces Accuracies achieved on \ac {SNLI} using $|r|$-dimensional sentence representations of gender-specific dimensions.}}{34}{table.5}}
\acronymused{SNLI}
\newlabel{tab:relevance_mf_nn}{{5}{34}{Accuracies achieved on \ac {SNLI} using $|r|$-dimensional sentence representations of gender-specific dimensions}{table.5}{}}
\acronymused{MLP}
\acronymused{SNLI}
\newlabel{eq:invert}{{8}{35}{}{equation.4.8}{}}
\acronymused{SNLI}
\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces Results in terms of accuracy of inverted gender-specific dimensions on \ac {SNLI} train and dev set.}}{35}{table.6}}
\acronymused{SNLI}
\newlabel{tab:inverted_mf_results_acc}{{6}{35}{Results in terms of accuracy of inverted gender-specific dimensions on \ac {SNLI} train and dev set}{table.6}{}}
\@writefile{lot}{\contentsline {table}{\numberline {7}{\ignorespaces Comparison of samples between their predictions based on the original and gender-inverted sentence representations.}}{36}{table.7}}
\newlabel{tab:inverted_mf_results_samples}{{7}{36}{Comparison of samples between their predictions based on the original and gender-inverted sentence representations}{table.7}{}}
\acronymused{SNLI}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.4}Other semantic dimensions}{37}{subsubsection.4.2.4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.5}Syntactic dimensions}{38}{subsubsection.4.2.5}}
\acronymused{POS}
\acronymused{POS}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Dimension \textit  {713} encoding verbs (left) and dimension \textit  {2020} encoding adjectives.}}{38}{figure.13}}
\newlabel{fig:find_syntax_vb}{{13}{38}{Dimension \textit {713} encoding verbs (left) and dimension \textit {2020} encoding adjectives}{figure.13}{}}
\acronymused{SNLI}
\acronymused{SNLI}
\acronymused{SNLI}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Dimension \textit  {757}, encoding the subjects (left), and dimension \textit  {1840} encoding objects (right) of sentences.}}{39}{figure.14}}
\newlabel{fig:find_syntax_subj_obj}{{14}{39}{Dimension \textit {757}, encoding the subjects (left), and dimension \textit {1840} encoding objects (right) of sentences}{figure.14}{}}
\acronymused{POS}
\acronymused{SNLI}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Insights on the sentence alignment}{40}{subsection.4.3}}
\newlabel{sec:insights_sent_alignment}{{4.3}{40}{Insights on the sentence alignment}{subsection.4.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.1}Alignment analysis on a single sample}{40}{subsubsection.4.3.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces Word alignments of an entailing sentence pair either by counting all shared dimensions (left) or only dimensions with at least a value of 0.2 (right).}}{41}{figure.15}}
\newlabel{fig:alignment_entailment_sample_general}{{15}{41}{Word alignments of an entailing sentence pair either by counting all shared dimensions (left) or only dimensions with at least a value of 0.2 (right)}{figure.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces Visualitation of an entailing sample with applied element-wise multiplication either using the mean (left) or maximum (right) product of all shared dimensions for each word pair.}}{41}{figure.16}}
\newlabel{fig:alignment_entailment_sample_mult}{{16}{41}{Visualitation of an entailing sample with applied element-wise multiplication either using the mean (left) or maximum (right) product of all shared dimensions for each word pair}{figure.16}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces Visualitation of a contradicting sample by counting meaningful shared dimensions (left) and meaningful distinct dimensions 8right) amongst pairs of words.}}{42}{figure.17}}
\newlabel{fig:alignment_contr_sample_general}{{17}{42}{Visualitation of a contradicting sample by counting meaningful shared dimensions (left) and meaningful distinct dimensions 8right) amongst pairs of words}{figure.17}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces Dimension-wise visualization of distinct information represented by \textit  {sitting} in the premise and \textit  {standing} in the hypothesis.}}{43}{figure.18}}
\newlabel{fig:contradiction_alignment_unshared_dimwise}{{18}{43}{Dimension-wise visualization of distinct information represented by \textit {sitting} in the premise and \textit {standing} in the hypothesis}{figure.18}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.2}Approach for a general alignment understanding}{43}{subsubsection.4.3.2}}
\newlabel{sec:approach_general_alignment_understanding}{{4.3.2}{43}{Approach for a general alignment understanding}{subsubsection.4.3.2}{}}
\acronymused{MLP}
\@writefile{lof}{\contentsline {figure}{\numberline {19}{\ignorespaces Visualitation of a sample sentence pair with explanatory guides for interpretation.}}{44}{figure.19}}
\newlabel{fig:sample_coverage_prob}{{19}{44}{Visualitation of a sample sentence pair with explanatory guides for interpretation}{figure.19}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.3}Entailment analysis}{44}{subsubsection.4.3.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {20}{\ignorespaces Visualization of 150 sentence pairs ($p$, $h$), correctly labelled as entailment.}}{45}{figure.20}}
\newlabel{fig:entailment_uninversed}{{20}{45}{Visualization of 150 sentence pairs ($p$, $h$), correctly labelled as entailment}{figure.20}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {21}{\ignorespaces Visualization of samples predicted as entailment (left) and neutral (right) after swapping $p$ and $h$.}}{45}{figure.21}}
\newlabel{fig:alignment_entailment_inversed}{{21}{45}{Visualization of samples predicted as entailment (left) and neutral (right) after swapping $p$ and $h$}{figure.21}{}}
\acronymused{SNLI}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.4}Neutral and contradiction analysis}{46}{subsubsection.4.3.4}}
\newlabel{sec:understanding_align_neutral_contr}{{4.3.4}{46}{Neutral and contradiction analysis}{subsubsection.4.3.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {22}{\ignorespaces Visualitazion of 150 sentence pairs ($p$, $h$) correctly labelled as \textit  {neutral} (left) and \textit  {contradiction} (right).}}{46}{figure.22}}
\newlabel{fig:neutr_contr_uninversed}{{22}{46}{Visualitazion of 150 sentence pairs ($p$, $h$) correctly labelled as \textit {neutral} (left) and \textit {contradiction} (right)}{figure.22}{}}
\citation{nie2017shortcut}
\acronymused{MLP}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Summarizing the insights on max-pooled sentence-representations}{47}{subsection.4.4}}
\acronymused{SNLI}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5}Identification of missing knowledge}{47}{subsection.4.5}}
\acronymused{SNLI}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.5.1}Approach}{47}{subsubsection.4.5.1}}
\acronymused{SNLI}
\acronymused{BoW}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.5.2}Results}{48}{subsubsection.4.5.2}}
\@writefile{lot}{\contentsline {table}{\numberline {8}{\ignorespaces Misclassified samples with gold label \textit  {contradiction}, predicted as \textit  {entailment}.}}{48}{table.8}}
\newlabel{tab:misclassified_orig_contr}{{8}{48}{Misclassified samples with gold label \textit {contradiction}, predicted as \textit {entailment}}{table.8}{}}
\acronymused{POS}
\citation{pantel2005inducing}
\citation{pantel2005inducing}
\citation{lecun2015deep}
\citation{sahlgren2008distributional}
\@writefile{lot}{\contentsline {table}{\numberline {9}{\ignorespaces Misclassified samples with gold label \textit  {entailment}, predicted as \textit  {contradiction}.}}{49}{table.9}}
\newlabel{tab:misclassified_orig_ent}{{9}{49}{Misclassified samples with gold label \textit {entailment}, predicted as \textit {contradiction}}{table.9}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.5.3}Conclusions}{49}{subsubsection.4.5.3}}
\acronymused{NLI}
\citation{glockner_acl18}
\citation{jia-liang:2017:EMNLP2017}
\@writefile{toc}{\contentsline {section}{\numberline {5}Additional SNLI test-set}{51}{section.5}}
\newlabel{sec:additional_snli_set}{{5}{51}{Additional SNLI test-set}{section.5}{}}
\@writefile{lot}{\contentsline {table}{\numberline {10}{\ignorespaces Correctly classified examples.}}{51}{table.10}}
\newlabel{table:correct_samples}{{10}{51}{Correctly classified examples}{table.10}{}}
\acronymused{NLI}
\acronymused{NLU}
\acronymused{NLU}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Goal of the new test set}{51}{subsection.5.1}}
\acronymused{SNLI}
\acronymused{SNLI}
\acronymused{SNLI}
\acronymused{NLU}
\citation{gururangan2018annotation}
\citation{dasgupta2018evaluating}
\citation{goldberg2017Apr}
\citation{bowman2015large}
\acronymused{NLU}
\acronymused{SNLI}
\acronymused{NLU}
\acronymused{SNLI}
\acronymused{SNLI}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Dataset}{52}{subsection.5.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.2.1}Creation of adversarial samples}{52}{subsubsection.5.2.1}}
\acronymused{SNLI}
\acronymused{SNLI}
\acronymused{RTE}
\acronymused{SNLI}
\acronymused{SNLI}
\citation{kruszewski2015so}
\@writefile{lot}{\contentsline {table}{\numberline {11}{\ignorespaces Examples from the newly generated test set.}}{53}{table.11}}
\newlabel{tab:new_testset_samples}{{11}{53}{Examples from the newly generated test set}{table.11}{}}
\acronymused{SNLI}
\acronymused{POS}
\citation{maccartney2007natural}
\citation{maccartney2007natural}
\acronymused{SNLI}
\acronymused{SNLI}
\citation{bowman2015large}
\citation{bowman2015large}
\@writefile{lot}{\contentsline {table}{\numberline {12}{\ignorespaces Comparison of co-hyponyms in upward-monotone and downward-monone sentences.}}{55}{table.12}}
\newlabel{tab:monoton_samples}{{12}{55}{Comparison of co-hyponyms in upward-monotone and downward-monone sentences}{table.12}{}}
\acronymused{SNLI}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.2.2}Validation}{55}{subsubsection.5.2.2}}
\AC@undonewlabel{acro:HIT}
\newlabel{acro:HIT}{{5.2.2}{55}{Validation}{section*.24}{}}
\acronymused{HIT}
\acronymused{HIT}
\acronymused{HIT}
\acronymused{HIT}
\acronymused{SNLI}
\citation{landis1977measurement}
\citation{bowman2015large}
\citation{gong2017natural}
\acronymused{HIT}
\acronymused{SNLI}
\acronymused{SNLI}
\acronymused{HIT}
\acronymused{HIT}
\@writefile{lof}{\contentsline {figure}{\numberline {23}{\ignorespaces Example of a \ac {HIT} in Amazon Mechanical Turk.}}{56}{figure.23}}
\acronymused{HIT}
\newlabel{fig:example_hit}{{23}{56}{Example of a \ac {HIT} in Amazon Mechanical Turk}{figure.23}{}}
\acronymused{HIT}
\acronymused{NLU}
\acronymused{SNLI}
\citation{gong2017natural}
\citation{nie2017shortcut}
\citation{chen2017enhanced}
\citation{parikh2016decomposable}
\citation{parikh2016decomposable}
\citation{nie2017shortcut}
\citation{chen2017enhanced}
\@writefile{lot}{\contentsline {table}{\numberline {13}{\ignorespaces Statistics of \ac {SNLI} testset compared with the newly generated testset.}}{57}{table.13}}
\acronymused{SNLI}
\newlabel{tab:newtest_stats}{{13}{57}{Statistics of \ac {SNLI} testset compared with the newly generated testset}{table.13}{}}
\acronymused{SNLI}
\acronymused{SNLI}
\acronymused{SNLI}
\acronymused{HIT}
\acronymused{SNLI}
\acronymused{SNLI}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Evaluation}{57}{subsection.5.3}}
\acronymused{SNLI}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.3.1}Experimental setup}{57}{subsubsection.5.3.1}}
\citation{mccarthy2004using}
\citation{chen2017natural}
\acronymused{biLSTM}
\acronymused{biLSTM}
\acronymused{SNLI}
\@writefile{lot}{\contentsline {table}{\numberline {14}{\ignorespaces Architectural comparison of tested neural models without external knowledge.}}{58}{table.14}}
\newlabel{tab:compare_architecture_models}{{14}{58}{Architectural comparison of tested neural models without external knowledge}{table.14}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.3.2}Models with external knowledge}{58}{subsubsection.5.3.2}}
\citation{parikh2016decomposable}
\citation{chen2017enhanced}
\citation{nie2017shortcut}
\citation{chen2017natural}
\acronymused{KIM}
\acronymused{SNLI}
\acronymused{MultiNLI}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.3.3}Results}{59}{subsubsection.5.3.3}}
\@writefile{lot}{\contentsline {table}{\numberline {15}{\ignorespaces Results of models on the new test set compared with the original \ac {SNLI} test set.}}{59}{table.15}}
\acronymused{SNLI}
\newlabel{tab:adv_results}{{15}{59}{Results of models on the new test set compared with the original \ac {SNLI} test set}{table.15}{}}
\acronymused{MultiNLI}
\acronymused{SNLI}
\acronymused{NLU}
\acronymused{MultiNLI}
\acronymused{KIM}
\acronymused{SNLI}
\acronymused{SNLI}
\acronymused{NLU}
\acronymused{SNLI}
\acronymused{KIM}
\acronymused{NLU}
\acronymused{KIM}
\citation{gururangan2018annotation}
\acronymused{KIM}
\acronymused{KIM}
\acronymused{NLU}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4}Analysis}{60}{subsection.5.4}}
\acronymused{MultiNLI}
\acronymused{SNLI}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.4.1}Accuracy by category}{60}{subsubsection.5.4.1}}
\acronymused{KIM}
\@writefile{lot}{\contentsline {table}{\numberline {16}{\ignorespaces Accuracy reached for the tested models for each category with assoziated sample words and the amount of instances.}}{60}{table.16}}
\newlabel{tab:acc_by_cat}{{16}{60}{Accuracy reached for the tested models for each category with assoziated sample words and the amount of instances}{table.16}{}}
\acronymused{SNLI}
\acronymused{SNLI}
\acronymused{SNLI}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.4.2}Impact on the word embeddings}{61}{subsubsection.5.4.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {24}{\ignorespaces Accuracy by cosine similarity reached by Decomposable Attention (without fine-tuned embeddings).}}{61}{figure.24}}
\newlabel{fig:decomp_acc_per_cos_sim}{{24}{61}{Accuracy by cosine similarity reached by Decomposable Attention (without fine-tuned embeddings)}{figure.24}{}}
\acronymused{SNLI}
\@writefile{lof}{\contentsline {figure}{\numberline {25}{\ignorespaces Accuracy by word freqency for Residual-Stacked Encoder and ESIM.}}{62}{figure.25}}
\newlabel{fig:esim_res_acc_by_freq}{{25}{62}{Accuracy by word freqency for Residual-Stacked Encoder and ESIM}{figure.25}{}}
\acronymused{SNLI}
\@writefile{lot}{\contentsline {table}{\numberline {17}{\ignorespaces Accuracy by the amount of similar samples in \ac {SNLI} train data for ESIM on contradicting samples.}}{62}{table.17}}
\acronymused{SNLI}
\newlabel{tab:esim_acc_by_sim_samples}{{17}{62}{Accuracy by the amount of similar samples in \ac {SNLI} train data for ESIM on contradicting samples}{table.17}{}}
\acronymused{ESIM}
\citation{williams2017broad}
\citation{shwartz2016adding}
\acronymused{MultiNLI}
\acronymused{SNLI}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5}Conclusion of the adversarial dataset}{63}{subsection.5.5}}
\acronymused{NLU}
\acronymused{NLI}
\acronymused{NLU}
\acronymused{SNLI}
\citation{nangia2017repeval}
\citation{bengio2013representation}
\citation{ruckle2018concatenated}
\citation{ruckle2018concatenated}
\@writefile{toc}{\contentsline {section}{\numberline {6}Approaches to incorporate WordNet information}{64}{section.6}}
\newlabel{sec:approaches_ext_res}{{6}{64}{Approaches to incorporate WordNet information}{section.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Methods}{64}{subsection.6.1}}
\acronymused{KIM}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.1.1}Drawbacks of using insights of max-pooled sentence representations}{64}{subsubsection.6.1.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.1.2}Fuse WordNet information within the embedding-layer}{64}{subsubsection.6.1.2}}
\acronymused{MLP}
\citation{bengio2013representation}
\citation{gururangan2018annotation}
\citation{gulccehre2016knowledge}
\citation{nangia2017repeval}
\AC@undonewlabel{acro:MSE}
\newlabel{acro:MSE}{{6.1.2}{65}{}{section*.25}{}}
\acronymused{MSE}
\newlabel{eq:loss_embd}{{13}{65}{}{equation.6.13}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.1.3}Fuse WordNet information within the sentence-representations}{65}{subsubsection.6.1.3}}
\newlabel{sec:mt_learning_intro}{{6.1.3}{65}{Fuse WordNet information within the sentence-representations}{subsubsection.6.1.3}{}}
\acronymused{NLU}
\acronymused{NLP}
\citation{vulic2017specialising}
\acronymused{NLI}
\@writefile{lof}{\contentsline {figure}{\numberline {26}{\ignorespaces Architecture of the Residual-Stacked Encoder with multitask learning for the sentence-representations.}}{66}{figure.26}}
\newlabel{fig:mt_architecture}{{26}{66}{Architecture of the Residual-Stacked Encoder with multitask learning for the sentence-representations}{figure.26}{}}
\acronymused{MLP}
\acronymused{MLP}
\acronymused{BoW}
\newlabel{eq:multitask_aggregate}{{14}{67}{}{equation.6.14}{}}
\acronymused{MLP}
\acronymused{MLP}
\acronymused{SNLI}
\acronymused{SNLI}
\acronymused{MLP}
\acronymused{MLP}
\acronymused{MLP}
\acronymused{MLP}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Extraction of WordNet data}{68}{subsection.6.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.2.1}Strategy to extract data}{68}{subsubsection.6.2.1}}
\newlabel{sec:used_wordnet_extract_strategy}{{6.2.1}{68}{Strategy to extract data}{subsubsection.6.2.1}{}}
\acronymused{SNLI}
\acronymused{SNLI}
\citation{ruckle2018concatenated}
\citation{chen2017enhanced}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.2.2}Final extracted data}{69}{subsubsection.6.2.2}}
\@writefile{lot}{\contentsline {table}{\numberline {18}{\ignorespaces Examples of extracted word-pairs ($w_1$,$w_2$) for both categories, being represented by the sentence containing $w_1$ ( thus $A$) or not (thus $B$).}}{69}{table.18}}
\newlabel{tab:examples_extracted_wn}{{18}{69}{Examples of extracted word-pairs ($w_1$,$w_2$) for both categories, being represented by the sentence containing $w_1$ ( thus $A$) or not (thus $B$)}{table.18}{}}
\acronymused{SNLI}
\acronymused{SNLI}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3}Evaluation}{69}{subsection.6.3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.3.1}Integrate WordNet using embeddings}{69}{subsubsection.6.3.1}}
\acronymused{SNLI}
\acronymused{SNLI}
\acronymused{SNLI}
\@writefile{lot}{\contentsline {table}{\numberline {19}{\ignorespaces Evaluation of experiments with additional information in the word-representations, compared to the Residual-Stacked Encoder\textsuperscript  {$\dagger $} (bottom).}}{70}{table.19}}
\newlabel{tab:eval_embeddings_added}{{19}{70}{Evaluation of experiments with additional information in the word-representations, compared to the Residual-Stacked Encoder\textsuperscript {$\dagger $} (bottom)}{table.19}{}}
\acronymused{ESIM}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.3.2}Integrate WordNet using multitask-learning}{70}{subsubsection.6.3.2}}
\newlabel{sec:eval_mt}{{6.3.2}{70}{Integrate WordNet using multitask-learning}{subsubsection.6.3.2}{}}
\acronymused{MLP}
\acronymused{MLP}
\acronymused{SNLI}
\@writefile{lot}{\contentsline {table}{\numberline {20}{\ignorespaces Evaluation of experiments using multitask-learning, compared with the Residual-Stacked Encoder\textsuperscript  {$\dagger $.}}}{70}{table.20}}
\newlabel{tab:mt_evaluation}{{20}{70}{Evaluation of experiments using multitask-learning, compared with the Residual-Stacked Encoder\textsuperscript {$\dagger $.}}{table.20}{}}
\acronymused{MLP}
\acronymused{MLP}
\acronymused{LSTM}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4}Analysis}{71}{subsection.6.4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.4.1}Integrate WordNet using embeddings}{71}{subsubsection.6.4.1}}
\@writefile{lot}{\contentsline {table}{\numberline {21}{\ignorespaces Accuracy per category for concatenated embeddings using Attract-Repel or Hyponyms-5.}}{71}{table.21}}
\newlabel{tab:detail_added_embds}{{21}{71}{Accuracy per category for concatenated embeddings using Attract-Repel or Hyponyms-5}{table.21}{}}
\citation{gururangan2018annotation}
\citation{dasgupta2018evaluating}
\acronymused{SNLI}
\@writefile{lof}{\contentsline {figure}{\numberline {27}{\ignorespaces Comparison of contradicting samples (different w.r.t. correctness from Residual-Stacked Encoder\textsuperscript  {$\dagger $}). for Hypernyms-5, by the amount of shared hypernym embeddings.}}{72}{figure.27}}
\newlabel{fig:analyse_hypern5}{{27}{72}{Comparison of contradicting samples (different w.r.t. correctness from Residual-Stacked Encoder\textsuperscript {$\dagger $}). for Hypernyms-5, by the amount of shared hypernym embeddings}{figure.27}{}}
\citation{chen2017natural}
\citation{chen2017natural}
\acronymused{SNLI}
\acronymused{SNLI}
\acronymused{NLU}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.4.2}Integrate WordNet using multitask-learning}{73}{subsubsection.6.4.2}}
\acronymused{MLP}
\@writefile{lot}{\contentsline {table}{\numberline {22}{\ignorespaces Accuracy per category for selected models using multitask-learning.}}{74}{table.22}}
\newlabel{tab:categories_mt}{{22}{74}{Accuracy per category for selected models using multitask-learning}{table.22}{}}
\@writefile{lot}{\contentsline {table}{\numberline {23}{\ignorespaces Accuracy per category of three selected multitask-learning experiments compared with Residual-Stacked Encoder\textsuperscript  {$\dagger $} on samples covered by extracted word-pairs.}}{74}{table.23}}
\newlabel{tab:eval_mt_data}{{23}{74}{Accuracy per category of three selected multitask-learning experiments compared with Residual-Stacked Encoder\textsuperscript {$\dagger $} on samples covered by extracted word-pairs}{table.23}{}}
\citation{mou2015natural}
\acronymused{KIM}
\acronymused{MLP}
\@writefile{lof}{\contentsline {figure}{\numberline {28}{\ignorespaces Aligned $p$ and $h$ for all contradicting sampes, covered by the fused WordNet information, correctly predicted (left) or mis-predicted (right).}}{75}{figure.28}}
\newlabel{fig:base_correct_incorrect_c}{{28}{75}{Aligned $p$ and $h$ for all contradicting sampes, covered by the fused WordNet information, correctly predicted (left) or mis-predicted (right)}{figure.28}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {29}{\ignorespaces Aligned $p$ and $h$, correctly predicted (left) and mis-predicted (right) for multitask-learned \textit  {300D-0.75 STD}.}}{76}{figure.29}}
\newlabel{fig:300d75_correct_incorrect_c}{{29}{76}{Aligned $p$ and $h$, correctly predicted (left) and mis-predicted (right) for multitask-learned \textit {300D-0.75 STD}}{figure.29}{}}
\citation{chen-EtAl:2017b:natural}
\citation{gururangan2018annotation}
\citation{chen-EtAl:2017b:natural}
\@writefile{lof}{\contentsline {figure}{\numberline {30}{\ignorespaces Aligned $p$ and $h$, of contradicting (left) and entailing (right) samples for multitask-learned \textit  {300D-0.75 max-pool}.}}{77}{figure.30}}
\newlabel{fig:masking_e_c}{{30}{77}{Aligned $p$ and $h$, of contradicting (left) and entailing (right) samples for multitask-learned \textit {300D-0.75 max-pool}}{figure.30}{}}
\acronymused{MLP}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.5}Summarizing experiments to incorporate WordNet}{77}{subsection.6.5}}
\acronymused{MLP}
\acronymused{KIM}
\acronymused{KIM}
\acronymused{SNLI}
\acronymused{SNLI}
\citation{sukhbaatar2015end}
\@writefile{toc}{\contentsline {section}{\numberline {7}Conclusion and future work}{79}{section.7}}
\acronymused{SNLI}
\acronymused{NLU}
\acronymused{NLI}
\acronymused{NLU}
\acronymused{SNLI}
\acronymused{KIM}
\bibdata{bib/literature}
\bibcite{balazs2017refining}{{1}{2017}{{Balazs et~al.}}{{Balazs, Marrese-Taylor, Loyola, and Matsuo}}}
\bibcite{bengio2013representation}{{2}{2013}{{Bengio et~al.}}{{Bengio, Courville, and Vincent}}}
\bibcite{Bishop2007}{{3}{2007}{{Bishop}}{{}}}
\bibcite{bos2005recognising}{{4}{2005}{{Bos and Markert}}{{}}}
\bibcite{bowman2015large}{{5}{2015}{{Bowman et~al.}}{{Bowman, Angeli, Potts, and Manning}}}
\bibcite{bowman2016fast}{{6}{2016}{{Bowman et~al.}}{{Bowman, Gauthier, Rastogi, Gupta, Manning, and Potts}}}
\bibcite{bromley1994signature}{{7}{1994}{{Bromley et~al.}}{{Bromley, Guyon, LeCun, S{\"a}ckinger, and Shah}}}
\bibcite{celikyilmaz2010enriching}{{8}{2010}{{Celikyilmaz et~al.}}{{Celikyilmaz, Hakkani-Tur, Pasupat, and Sarikaya}}}
\bibcite{chatzikyriakidis2017overview}{{9}{2017}{{Chatzikyriakidis et~al.}}{{Chatzikyriakidis, Cooper, Dobnik, and Larsson}}}
\bibcite{chen2017natural}{{10}{2017{a}}{{Chen et~al.}}{{Chen, Zhu, Ling, and Inkpen}}}
\bibcite{chen-EtAl:2017b:natural}{{11}{2018}{{Chen et~al.}}{{Chen, Zhu, Ling, Inkpen, and Wei}}}
\bibcite{chen2017enhanced}{{12}{2017{b}}{{Chen et~al.}}{{Chen, Zhu, Ling, Wei, Jiang, and Inkpen}}}
\bibcite{chen2017recurrent}{{13}{2017{c}}{{Chen et~al.}}{{Chen, Zhu, Ling, Wei, Jiang, and Inkpen}}}
\bibcite{cheng2016long}{{14}{2016}{{Cheng et~al.}}{{Cheng, Dong, and Lapata}}}
\bibcite{clark2016combining}{{15}{2016}{{Clark et~al.}}{{Clark, Etzioni, Khot, Sabharwal, Tafjord, Turney, and Khashabi}}}
\bibcite{cooper1996using}{{16}{1996}{{Cooper et~al.}}{{Cooper, Crouch, Van~Eijck, Fox, Van~Genabith, Jaspars, Kamp, Milward, Pinkal, Poesio et~al.}}}
\bibcite{dagan2000contextual}{{17}{2000}{{Dagan}}{{}}}
\bibcite{dagan2009recognizing}{{18}{2009}{{Dagan et~al.}}{{Dagan, Dolan, Magnini, and Roth}}}
\bibcite{dagan2006pascal}{{19}{2006}{{Dagan et~al.}}{{Dagan, Glickman, and Magnini}}}
\bibcite{dasgupta2018evaluating}{{20}{2018}{{Dasgupta et~al.}}{{Dasgupta, Guo, Stuhlm{\"u}ller, Gershman, and Goodman}}}
\bibcite{faruqui2015retrofitting}{{21}{2015}{{Faruqui et~al.}}{{Faruqui, Dodge, Jauhar, Dyer, Hovy, and Smith}}}
\bibcite{ghaeini2018dr}{{22}{2018}{{Ghaeini et~al.}}{{Ghaeini, Hasan, Datla, Liu, Lee, Qadir, Ling, Prakash, Fern, and Farri}}}
\bibcite{glockner_acl18}{{23}{2018}{{Glockner et~al.}}{{Glockner, Shwartz, and Goldberg}}}
\bibcite{goldberg2017Apr}{{24}{2017}{{Goldberg}}{{}}}
\bibcite{gong2017natural}{{25}{2017}{{Gong et~al.}}{{Gong, Luo, and Zhang}}}
\bibcite{graves2005framewise}{{26}{2005}{{Graves and Schmidhuber}}{{}}}
\bibcite{gulccehre2016knowledge}{{27}{2016}{{G{\"u}l{\c {c}}ehre and Bengio}}{{}}}
\bibcite{gurevych2012uby}{{28}{2012}{{Gurevych et~al.}}{{Gurevych, Eckle-Kohler, Hartmann, Matuschek, Meyer, and Wirth}}}
\bibcite{gurevych2016linked}{{29}{2016}{{Gurevych et~al.}}{{Gurevych, Eckle-Kohler, and Matuschek}}}
\bibcite{gururangan2018annotation}{{30}{2018}{{Gururangan et~al.}}{{Gururangan, Swayamdipta, Levy, Schwartz, Bowman, and Smith}}}
\bibcite{hochreiter1997long}{{31}{1997}{{Hochreiter and Schmidhuber}}{{}}}
\bibcite{hu2016deep}{{32}{2016}{{Hu et~al.}}{{Hu, Yang, Salakhutdinov, and Xing}}}
\bibcite{ide2001american}{{33}{2001}{{Ide and Macleod}}{{}}}
\bibcite{ide2004american}{{34}{2004}{{Ide and Suderman}}{{}}}
\bibcite{ide2006integrating}{{35}{2006}{{Ide and Suderman}}{{}}}
\bibcite{im2017distance}{{36}{2017}{{Im and Cho}}{{}}}
\bibcite{jia-liang:2017:EMNLP2017}{{37}{2017}{{Jia and Liang}}{{}}}
\bibcite{Jurafsky2008May}{{38}{2008}{{Jurafsky and Martin}}{{}}}
\bibcite{scitail}{{39}{2018}{{Khot et~al.}}{{Khot, Sabharwal, and Clark}}}
\bibcite{kingma2014adam}{{40}{2014}{{Kingma and Ba}}{{}}}
\bibcite{kruszewski2015so}{{41}{2015}{{Kruszewski and Baroni}}{{}}}
\bibcite{landis1977measurement}{{42}{1977}{{Landis and Koch}}{{}}}
\bibcite{lecun2015deep}{{43}{2015}{{LeCun et~al.}}{{LeCun, Bengio, and Hinton}}}
\bibcite{levy2015improving}{{44}{2015}{{Levy et~al.}}{{Levy, Goldberg, and Dagan}}}
\bibcite{liu2015learning}{{45}{2015}{{Liu et~al.}}{{Liu, Jiang, Wei, Ling, and Hu}}}
\bibcite{maccartney2008phrase}{{46}{2008}{{MacCartney et~al.}}{{MacCartney, Galley, and Manning}}}
\bibcite{maccartney2007natural}{{47}{2007}{{MacCartney and Manning}}{{}}}
\bibcite{marcus1993building}{{48}{1993}{{Marcus et~al.}}{{Marcus, Marcinkiewicz, and Santorini}}}
\bibcite{marelli2014semeval}{{49}{2014}{{Marelli et~al.}}{{Marelli, Bentivogli, Baroni, Bernardi, Menini, and Zamparelli}}}
\bibcite{mccarthy2004using}{{50}{2004}{{McCarthy et~al.}}{{McCarthy, Koeling, Weeds, and Carroll}}}
\bibcite{mikolov2013distributed}{{51}{2013{a}}{{Mikolov et~al.}}{{Mikolov, Sutskever, Chen, Corrado, and Dean}}}
\bibcite{mikolov2013linguistic}{{52}{2013{b}}{{Mikolov et~al.}}{{Mikolov, Yih, and Zweig}}}
\bibcite{miller1995wordnet}{{53}{1995}{{Miller}}{{}}}
\bibcite{mou2015natural}{{54}{2015}{{Mou et~al.}}{{Mou, Men, Li, Xu, Zhang, Yan, and Jin}}}
\bibcite{mrkvsic2017semantic}{{55}{2017}{{Mrk{\v {s}}i{\'c} et~al.}}{{Mrk{\v {s}}i{\'c}, Vuli{\'c}, S{\'e}aghdha, Leviant, Reichart, Ga{\v {s}}i{\'c}, Korhonen, and Young}}}
\bibcite{munkhdalai2017neural}{{56}{2017}{{Munkhdalai and Yu}}{{}}}
\bibcite{murphy2003semantic}{{57}{2003}{{Murphy}}{{}}}
\bibcite{nangia2017repeval}{{58}{2017}{{Nangia et~al.}}{{Nangia, Williams, Lazaridou, and Bowman}}}
\bibcite{nie2017shortcut}{{59}{2017}{{Nie and Bansal}}{{}}}
\bibcite{pantel2005inducing}{{60}{2005}{{Pantel}}{{}}}
\bibcite{parikh2016decomposable}{{61}{2016}{{Parikh et~al.}}{{Parikh, T{\"a}ckstr{\"o}m, Das, and Uszkoreit}}}
\bibcite{pennington2014glove}{{62}{2014}{{Pennington et~al.}}{{Pennington, Socher, and Manning}}}
\bibcite{peters2018deep}{{63}{2018}{{Peters et~al.}}{{Peters, Neumann, Iyyer, Gardner, Clark, Lee, and Zettlemoyer}}}
\bibcite{prakash2007learning}{{64}{2007}{{Prakash et~al.}}{{Prakash, Jurafsky, and Ng}}}
\bibcite{resnik1995using}{{65}{1995}{{Resnik}}{{}}}
\bibcite{rocktaschel2015reasoning}{{66}{2015}{{Rockt{\"a}schel et~al.}}{{Rockt{\"a}schel, Grefenstette, Hermann, Ko{\v {c}}isk{\`y}, and Blunsom}}}
\bibcite{ruckle2018concatenated}{{67}{2018}{{R{\"u}ckl{\'e} et~al.}}{{R{\"u}ckl{\'e}, Eger, Peyrard, and Gurevych}}}
\bibcite{sahlgren2008distributional}{{68}{2008}{{Sahlgren}}{{}}}
\bibcite{shen2018reinforced}{{69}{2018}{{Shen et~al.}}{{Shen, Zhou, Long, Jiang, Wang, and Zhang}}}
\bibcite{shwartz2016adding}{{70}{2016}{{Shwartz and Dagan}}{{}}}
\bibcite{shwartz2015learning}{{71}{2015}{{Shwartz et~al.}}{{Shwartz, Levy, Dagan, and Goldberger}}}
\bibcite{suchanek2007yago}{{72}{2007}{{Suchanek et~al.}}{{Suchanek, Kasneci, and Weikum}}}
\bibcite{sukhbaatar2015end}{{73}{2015}{{Sukhbaatar et~al.}}{{Sukhbaatar, Weston, Fergus et~al.}}}
\bibcite{tatu2005semantic}{{74}{2005}{{Tatu and Moldovan}}{{}}}
\bibcite{tay2017compare}{{75}{2017}{{Tay et~al.}}{{Tay, Tuan, and Hui}}}
\bibcite{vulic2017specialising}{{76}{2017}{{Vuli{\'c} and Mrk{\v {s}}i{\'c}}}{{}}}
\bibcite{vulic2017morph}{{77}{2017}{{Vuli{\'c} et~al.}}{{Vuli{\'c}, Mrk{\v {s}}i{\'c}, Reichart, S{\'e}aghdha, Young, and Korhonen}}}
\bibcite{welbl2017crowdsourcing}{{78}{2017}{{Welbl et~al.}}{{Welbl, Liu, and Gardner}}}
\bibcite{williams2017broad}{{79}{2017}{{Williams et~al.}}{{Williams, Nangia, and Bowman}}}
\bibcite{xu2014rc}{{80}{2014}{{Xu et~al.}}{{Xu, Bai, Bian, Gao, Wang, Liu, and Liu}}}
\bibcite{yang2017character}{{81}{2017}{{Yang et~al.}}{{Yang, Costa-juss{\`a}, and Fonollosa}}}
\bibcite{young2014image}{{82}{2014}{{Young et~al.}}{{Young, Lai, Hodosh, and Hockenmaier}}}
\bibcite{zesch2008extracting}{{83}{2008}{{Zesch et~al.}}{{Zesch, M{\"u}ller, and Gurevych}}}
\citation{nie2017shortcut}
\acronymused{RNN}
\acronymused{RNN}
\acronymused{POS}
\acronymused{HIT}
\citation{nie2017shortcut}
\acronymused{SNLI}
\acronymused{MultiNLI}
\acronymused{SNLI}
\acronymused{SNLI}
\acronymused{SNLI}
\acronymused{SNLI}
\acronymused{SNLI}
\global\@namedef{scr@dte@section@lastmaxnumwidth}{10.85753pt}
\global\@namedef{scr@dte@subsection@lastmaxnumwidth}{19.68993pt}
\global\@namedef{scr@dte@subsubsection@lastmaxnumwidth}{28.86392pt}
\global\@namedef{scr@dte@table@lastmaxnumwidth}{16.63193pt}
\global\@namedef{scr@dte@figure@lastmaxnumwidth}{16.63193pt}
