\section{Introduction}
In recent years neural networks again gained a lot of popularity for many machine learning tasks, including the field of \ac{NLP}. While previous generation solutions heavily depended on handcrafted features, these models are capable of learning meaningful feature representations automatically \citep{bengio2013representation}, thus avoiding time consuming process of feature-engineering. For the most part, neural networks solely rely on distributed word representations, also known refered to as word-embeddings, like word2vec \citep{mikolov2013distributed} or GloVe \citep{pennington2014glove} and typically learn fixed-length dense vector representations for the input text. While they provie strong generalization capabilities, they fail to capture simple world-knowledge \citep{celikyilmaz2010enriching} and even have trouble differentiationg between mutually exclusive words, if they generally are used in similar contexts \citep{vulic2017morph}. As opposed to that, traditional approaches execcively made usage of lexical resources containing relational and factual information about words and entities, thus providing a huge amount of ready-to-sue knowledge bases. Intuitivly, combining both worlds by integrating existing knowldge bases into neural networks should even further improve these models, due to a more sophisticated \ac{NLU}. This is analogue tothe way humans understand text, by having a solid understanding of the world, that influences the subjective interpretation of every word within a sentence. Given the sentence ``The official language in the USA is English.'' an average human can conclude that the official language of \textit{New York} also is English, knowing that \textit{New York} is within the \textit{USA}.
\newline

To improve the \ac{NLU} of neural models for \ac{NLP} we address the mentioned problems by analysing the sentence representations of a state-of-the-art model and identifying knowledge that is captured or not captured using state-of-the-art strategies without external resources. We show those state-of-the-art models are limited in their generalization ability and fail to capture simple inferences. To overcome this problem, we evaluate how additional knowledge from exxternal resources could be inferred neural networks. While our aim is to provide generally applicable results, we base our experiments on the task of \ac{NLI} \citep{bowman2015large}, also known as \ac{RTE} \citep{dagan2006pascal}. As this is known to be a fundamental task for \ac{NLU} \citep{maccartney2007natural}, insights gained here can improve other tasks of \ac{NLP} that indirectly depend on it. 

\paragraph*{Structure of the Thesis}
While we explain relevant techniques and concepts, we expect the reader to have a basic understanding of common machine-learning practices, neural networks, including basic network architectures like \ac{LSTM} or \ac{RNN}, and \ac{NLP} in general. This thesis is structured in the following manner:
\begin{itemize}
\item Section ยง\ref{sec:basics} is used to give definitions for \ac{NLI} and relevant word-relations. We further give a detailed description about the architecture and traning of the state-of-the-art model, that we use throughout all our experiments.
\item In section ยง\ref{sec:related_work} we introduce recently publish relevant datasets for \ac{NLI}and discuss several strategies proved to be successful. In addition we show a selectionn of lexical resources that contain relevant information to improve the \ac{NLU} of neural models and various strategies that have been applied to integrate them.
\item We analyse how the information of a natural language text is encoded within the sentence representation of a neural model and give insights on how the model uses it in section ยง\ref{sec:understanding}.
\item We derive a new testset from a major dataset for \ac{NLI}, demonstrating the poor generalization abilities of state-of-the-art models in section ยง\ref{sec:additional_snli_set}.
\item Based on the new data we evaluate whether external resources are helpful for the task using advanced embeddings and multitask-learning.
\end{itemize}