\section{Introduction}
In recent years neural networks again gained a lot of popularity for many machine learning tasks, including the field of \ac{NLP}. While previous generation solutions heavily depended on handcrafted features, these models are capable of learning meaningful feature representations automatically \citep{bengio2013representation}, thus avoiding the time consuming process of feature-engineering. For the most part, neural networks solely rely on distributed word representations, like word2vec \citep{mikolov2013distributed} or GloVe \citep{pennington2014glove} and typically learn fixed-length dense vector representations for the input text. While those distributed word-vectors provide strong generalization capabilities, they fail to capture simple world-knowledge \citep{celikyilmaz2010enriching} and even have trouble differentiating between mutually exclusive words, if they generally are used in similar contexts \citep{vulic2017morph}. As opposed to neural models relying on distributed representations, traditional approaches execively made use of lexical resources containing relational and factual information about words and entities. Despite being well studied and containing valuabe information that is not present within distributed word-vectors, those lexical resources are for the most part ignored by neural approaches. Intuitively, combining the approaches with complementary strengths, the generalization power of distributed representations and the knowledge-rich structures of lexical resources, may further lead to better \ac{NLU} capabilities of models and thus increase the performance on a wide variety of tasks.

\subsection{Goal of this thesis}
The goal of this work is to identify directions to incorporate this external knowledge into neural models. This is highly aligned with the way, humans understand text, by having a solid understanding of the world, that influences the subjective interpretation of every word within a sentence. Given the sentence ``The official language in the USA is English.'' an average human can conclude that the official language of ``New York'' also is English, knowing that ``New York'' is within the ``USA''. We try to solve this problem on the task of \ac{NLI} \citep{bowman2015large}, also known as \ac{RTE} \citep{dagan2006pascal}. As this is known to be a fundamental task for \ac{NLU} \citep{maccartney2007natural}, insights gained here can improve other tasks of \ac{NLP} that indirectly depend on it. Specifically, we try to incorporate external knowledge into the sentence-representation of a state-of-the-art model for \ac{NLI}. Therefore, we start by analysing how sentences are encoded within that model, identifying the role of different dimensions and how the encoded values within those dimensions serve the final prediction. To show the relevance of the knowledge we infer, we create an additional test-set for \ac{NLI}, that can be considered to be easily solvable based on the train-data, if the predictions indeed are based on a proper \ac{NLU}. On this new dataset we finally evaluate two strategies to infer the required knowledge, either by adding it to the word-representations or, by fusing it into the sentence representation.

\subsection{Structure of this thesis}
While we explain relevant techniques and concepts, we expect the reader to have a basic understanding of common machine-learning practices, neural networks (including basic network architectures like \ac{LSTM} or \ac{RNN}) and \ac{NLP} in general. Opinions expressed within the thesis are those of the author and may not necessarily correspond to the opinions of other participators, even though everything is expressed using ``we''. In some sections, especially with mathematical equations, we assign meanings to single symbols, written in italic like $p$ or $h$. Unless we specifically mention that those will be identical for the remainder of the full thesis, we may re-define the meaning of those symbols in later sections. The code for all experiments is available on github\footnote{\href{https://github.com/Max216/ThesisPKinDL}{https://github.com/Max216/ThesisPKinDL}}.
\newline

\noindent
We give a quick overview of the content within this thesis:
\begin{itemize}
\item Section §\ref{sec:basics} introduces the task of \ac{NLI} as well as the state-of-the-art model we use throughout most of the experiments. Additionally, lexical semantic relations, which play a crucial role for this work, are defined.
\item In Section §\ref{sec:related_work} we introduce relevant datasets for \ac{NLI} and discuss several neural approaches, proved to be successful. In addition, we show and describe lexical resources, that contain relevant information to improve the \ac{NLU} of neural models, as well as various strategies that have been applied to integrate them.
\item We analyse how the information of a natural language text is encoded within the sentence representation of a neural model and give insights on how the model uses it in Section §\ref{sec:understanding}.
\item We derive a new testset from a major dataset for \ac{NLI}, demonstrating the poor generalization abilities of state-of-the-art models without external knowledge in Section §\ref{sec:additional_snli_set}.
\item Based on the new testset, we evaluate strategies to incorporate external knowledge into the sentence-representations, by either fusing the knowledge with the word-representations or directly into the sentence-representations using multitask-learning in Section §\ref{sec:approaches_ext_res}.
\end{itemize}