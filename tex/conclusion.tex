\section{Conclusion and future work}
In this work we showed at the sampe of \ac{SNLI}, that even though being intended to improve \ac{NLU}, the high performance gained by state-of-the-art models for \ac{NLI} does not reflect the actual \ac{NLU} capabilities. All models performed significantly lower on our adversarial dataset, derived from the original train-set but excluded  \ac{SNLI}-specific patterns. Even though \ac{KIM} performed quite well, future work may find ways to integrate more data or methods to deal with lexical inferences in context to improve the performance, which has not yet met the limit. We attempted to improvce the performance on this new dataset using WordNet information for a sentence-encoding model. Here, we leveraged from the max-pooled sentence-encoding and showed that this can be used to understand the dimensional values and sentence-representations, generated by the model. In addition to showing that this information can indeed be used to change the representations in a meaningful way, our inisghts gained here, have also shown to be useful for an intrinsic analysis of the sentence-representations of the experiments incorporating WordNet. Future work can develop deper insights on these representations on a broader range of data and models, to enable more meaningful analysis or even adaptions. Finally we evaluated in our approaches to incorporate WordNet, that this should be done in a very easy to identify way, in order to be relevant enough to overcome diminant patterns in a dataset. Especially for sentence-encoding models this seems very challenging at the moment, future work may leverage from structures like memory networks \citep{sukhbaatar2015end}, to do so.

\section{Acknowledgements}
Special Thanks to Yoav Goldberg and Vered Shwartz