\section{Approaches to incorporate WordNet information}
\subsection{Extraction of WordNet data}
\subsection{Integrating information into word-embeddings}
\subsubsection{Motivation}
\citep{rubinstein2015well} show something that distributional embeddings not always good (reread)
\subsubsection{Concatenating pre-trained word-embeddings}
\subsubsection{Concatenation categorical information}
\subsubsection{Analysis}
\subsection{Multitask Learning}
\citep{levy2015improving} show that embeddings not necessarily (reread)
\subsubsection{Motivation}
\subsubsection{Architecture}
\subsubsection{Approaches}
\paragraph{Different sizes of multitask MLP}
\paragraph{Introducing Dropout}
\paragraph{Introducing an additional shared layer}
\paragraph{Fixing multitasking network during training}
\paragraph{Focusing on original words within sentence representation}
\subsubsection{Analysis}
\subsubsection{Evaluation}

cite: An Overview of Multi-Task Learning
in Deep Neural Networks