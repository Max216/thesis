\section{Approaches to incorporate WordNet information}
Having the dataset of the previous section, we next try to improve our latest re-implementation Residual-Stacked Encoder\textsuperscript{$\dagger$} using WordNet. 
\subsection{Methods}
Unlike \ac{KIM}, which has shown an intuitive and successful strategy of incorporating WordNet into a neural network, the Residual-Stacked Encoder does not use inter-sentence attention. Without changing this, we therefore cannot likewise align words of $p$ with words of $h$ to identify their WordNet relation. We intend to leave the model with the plain sentence-encoding architecture, targeting the incorporation of external resources for general sentence representations thus by encoding each sentence individually \citep{nangia2017repeval}. Naturally, not being able to align words with each other poses a new difficulty, since the relations of WordNet are defined between two senses. Subsequentially, we apply other strategies, than directly encoding the relation of two words (or senses), explained below.
\subsubsection{Drawbacks of using insights of max-pooled sentence representations}
In Section ยง\ref{sec:understanding} we gained valid insights on the sentence representations and showed that these sucessfully can be used to change the meaning of sentence representations. Following these conclusions, a possible strategy is, to train the model in a way, that antonyms or co-hyponyms result in distinct high dimensions within the sentence representation, synonyms having the same high values and hypernyms a subset of hyponyms. Knowing reasonable values for each values within the dimensions, this could be broken down to a simple regression problem. Since we did not find an elegant way to naturally include this into the loss function, two possibilities exist:
\begin{itemize}
\item \textbf{Manually define the $\xi$ of dimensions:} The values for each word are defined before the training, w.r.t. the lexical relations that are shared amongst them, using automatic optimization techniques. The loss is defined w.r.t. those defined values for each word within a sentence.
\item \textbf{Regularily analyse the $\xi$ of dimensions:} Instead of defining the $\xi$ of each dimension beforehand, it can be gained by an anaylse step, similar to the one conducted for this work, every iteration. Knowing the dimensions, how they are already used by the model, they can slightly be adapted such that our wanted criteria is met. For instance, let the words $w_1$ and $w_2$ share a common close hypernym, thus both are co-hyponyms. After identifying the dimensions, that result in a high value coming from both words, some of tose dimensions are reserved for $w_1$ only, while others are reserved for $w_2$ only. This may also be achieved automatically, if dimensions are analysed for their high-valued words w.r.t. WordNet relations.
\end{itemize}
Both approaches severely lack the possibility of encoding words depending on their context into the sentence representation. While we gained some insights on the sentence-representations, it is insufficient to define them in a reasonable way. Even if this would be possible, it is not desired. Both strategies, especially the first, highly resemble traditional feature-engineering. Since the automatic feature selection is one of the key strengths of neural models \citep{bengio2013representation}, those strategies would rather be similar to a step backward than forward. Instead we identify to potential strategies, that are simpler to implement and would result in a broader applicability, not being tied to max-pooled sentence representations.

\subsubsection{Fuse WordNet information within the embedding-layer}
Additional information within the word-representations has the high advantage, of being very gerneral applicable. Following \cite{ruckle2018concatenated} we do not use exclusively retrained or adjusted word-embeddings. Instead, for each word $w$ we lookup the according word-vector within the original distributed GloVe embeddings and concatenate it with the corresponding word-vector of the same $w$ from the additional word-embeddings. If no corresponding vector for $w$ is present within those, we concatenate an zero-valued vector of the same dimensionality. Additional to doing this experiment with the mono-lingual attract-repel vectors, provided by \cite{ruckle2018concatenated}, we use two different word-vecor sources.

\paragraph*{Overfitting WordNet}
We apply a simple method to create addtitional word vectors $v$ that are similar for the words $w_1$ and $w_2$, if they are synonyms within WordNet, and different if $w_1$ and $w_2$ are antonyms or co-hyponyms. For this we extract samples ($w_1$, $w_2$, \texttt{relation}), whereas $w_1$ and $w_2$ are lemmas, that are linked via \texttt{relation} within WordNet, represented by their original GloVe embeddings. Using a two layer \ac{MLP}, we map each word-vector $w \in \mathbb{R}^{300}$ to $v \in \mathbb{R}^{20}$. In our last layer we apply tangens-hyperbolicus as non-linearity, to squeeze all values $v^i$ within $v$, with $v^i$ being the $i$th value within $v$, are in an appropriate range:  $ \forall i: [i \in \{x \in \mathbb{N} | x < 20\} \Rightarrow v^i \in \{x \in \mathbb{R} | -1 < x < 1\}]$. Let $w \in \mathbb{R}^{300 \times 1}$ be the original word-embedding, $W_1 \in \mathbb{R}^{100 \times 300}$ and $b_1 \in \mathbb{R}^{100 \times 1}$ the weight matrix and bias of the first layer, and $W_2 \in \mathbb{R}^{20 \times 100}$ and $b_2 \in \mathbb{R}^{20 \times 1}$ of the second layer respectively. The new word-vector $v$ is calculated as:
\begin{equation}
v = \tanh(W_2 \text{ reLU}(W_1w + b_1) + b_2)
\end{equation}
We optimize the representations $v_1$ and $v_2$, coming from ($w_1$, $w_2$, \texttt{relation}) using \ac{MSE} with the Eucledian Distance, which should be high, if the relation indicates, $w_1$ and $w_2$ are mutually exclusive, and low, if both are synonyms. Thus, we define $\theta=0$ for synonyms, and $\theta =10$, for antonyms and co-hyponyms (we bound the difference to $\frac{|v|}{2}$, with $|v|$ being the amount of dimensions of the new word-vectors, as it creates sufficiently distinct vectors):
\begin{equation}
\text{loss} = \frac{1}{2}\Bigg( \sqrt{\sum^{20}_{i=1}(v^i_1 - v^i_2)^2} - \theta\Bigg)^2
\end{equation}
\paragraph*{Adding categorical information}
\subsubsection{Fuse WordNet information within the sentence-representations}

\subsection{Extraction of WordNet data}
\subsection{Integrating information into word-embeddings}
\subsubsection{Motivation}
\citep{rubinstein2015well} show something that distributional embeddings not always good (reread)
\subsubsection{Concatenating pre-trained word-embeddings}
\subsubsection{Concatenation categorical information}
\subsubsection{Analysis}
\subsection{Multitask Learning}
\citep{levy2015improving} show that embeddings not necessarily (reread)
\subsubsection{Motivation}
\subsubsection{Architecture}
\subsubsection{Approaches}
\paragraph{Different sizes of multitask MLP}
\paragraph{Introducing Dropout}
\paragraph{Introducing an additional shared layer}
\paragraph{Fixing multitasking network during training}
\paragraph{Focusing on original words within sentence representation}
\subsubsection{Analysis}
\subsubsection{Evaluation}

cite: An Overview of Multi-Task Learning
in Deep Neural Networks